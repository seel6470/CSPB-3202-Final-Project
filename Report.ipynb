{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8866730-fe6d-4e2f-ba9d-61204c3f1aa0",
   "metadata": {},
   "source": [
    "# Beating Super Mario Bros (Level 1-1) With Reinforcement Learning\n",
    "### Seth Ely - CSPB 3202 Summer 2024\n",
    "<ol type=\"I\">\n",
    "<li><a href='#short-overview'>Short Overview</a></li>\n",
    "<li><a href='#approach'>Approach</a>\n",
    "  <ol type=\"A\">\n",
    "    <li><a href='#random-agent'>Random Agent</a></li>\n",
    "    <li><a href='#heuristic-agent'>Heuristic Agent</a></li>\n",
    "    <li><a href='#ppo-agent'>PPO Agent</a></li>\n",
    "    <li><a href='#q-learning-agent'>Q-Learning Agent</a></li>\n",
    "  </ol>\n",
    "</li>\n",
    "<li><a href='#results'>Results</a></li>\n",
    "<li><a href='#conclusion'>Conclusion</a></li>\n",
    "<li><a href='#references'>References</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc2037-5366-430d-bb52-2170e163d598",
   "metadata": {},
   "source": [
    "GitHub Repository for this project can be found at https://github.com/seel6470/CSPB-3202-Final-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99cf0f-1b67-44ab-a413-64bd96974842",
   "metadata": {},
   "source": [
    "<a id='short-overview'></a>\n",
    "## Short Overview\n",
    "\n",
    "For my project, I chose to teach a learning model to play the original Super Mario Bros. game for the NES. I utilized a library created by Christian Kauten called gym-super-mario-bros, which provides an OpenAI Gym environment using the nes-py emulator [(Kauten, 2018)](#references). The challenge is to beat as many levels as possible in the original Mario game for NES with the following rules of the game.\n",
    "\n",
    "To start, I will create a random agent (choosing actions randomly), then a heuristic agent that chooses actions based on a given strategy, and finally two reinforcement learning models to compare against each other: a proximal policy optimization (PPO) model and a Q-Learning model. \n",
    "\n",
    "The main difference between PPO and Q-Learning will be in their approach to policy optimization and exploration. PPO is a policy-based method that directly optimizes actions, mapping the best action to take for any given game state. Q-Learning is value-based, focusing on learning state-action values. Because of this, PPO is better at determining continuous action spaces, whereas Q-Learning is better suited for discrete action spaces. Since Super Mario Bros. would represent a continuous action space with fluid movement between frames, I would anticipate the PPO model to be more effective, but it will be interesting to compare the results of each.\n",
    "\n",
    "The goal will be to create stable learning environments for both of these models and see how each agent performs in the game environment.\n",
    "\n",
    "### Rules of the Game\n",
    "\n",
    "The goal of the game is to avoid enemies and pits to reach the end of each level. One hit and Mario loses a life, starting over from the nearest checkpoint. Power-ups provide Mario an additional hit. The following page from the original game manual outlines the inputs Mario receives for the game:\n",
    "\n",
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/controls.jpg)\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "The game environment takes these controls and creates the following action lists that can be used within the environment wrapper:\n",
    "\n",
    "```python\n",
    "# actions for the simple run right environment\n",
    "RIGHT_ONLY = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for very simple movement\n",
    "SIMPLE_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for more complex movement\n",
    "COMPLEX_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "    ['left', 'A'],\n",
    "    ['left', 'B'],\n",
    "    ['left', 'A', 'B'],\n",
    "    ['down'],\n",
    "    ['up'],\n",
    "]\n",
    "```\n",
    "\n",
    "The environment can also determine the following keys for the gamestate:\n",
    "\n",
    "| Key       | Type | Description                                |\n",
    "|-----------|------|--------------------------------------------|\n",
    "| coins     | int  | The number of collected coins              |\n",
    "| flag_get  | bool | True if Mario reached a flag or ax         |\n",
    "| life      | int  | The number of lives left, i.e., {3, 2, 1}  |\n",
    "| score     | int  | The cumulative in-game score               |\n",
    "| stage     | int  | The current stage, i.e., {1, ..., 4}       |\n",
    "| status    | str  | Mario's status, i.e., {'small', 'tall', 'fireball'} |\n",
    "| time      | int  | The time left on the clock                 |\n",
    "| world     | int  | The current world, i.e., {1, ..., 8}       |\n",
    "| x_pos     | int  | Mario's x position in the stage (from the left) |\n",
    "| y_pos     | int  | Mario's y position in the stage (from the bottom) |\n",
    "\n",
    "Additionally, the environment utilizes the following parameters for the reward function:\n",
    "\n",
    "1. **v**: the difference in agent x values between states\n",
    "   - In this case, this is instantaneous velocity for the given step\n",
    "   - \\( v = x1 - x0 \\)\n",
    "     - \\( x0 \\) is the x position before the step\n",
    "     - \\( x1 \\) is the x position after the step\n",
    "   - Moving right \\( \\implies v > 0 \\)\n",
    "   - Moving left \\( \\implies v < 0 \\)\n",
    "   - Not moving \\( \\implies v = 0 \\)\n",
    "\n",
    "2. **c**: the difference in the game clock between frames\n",
    "   - The penalty prevents the agent from standing still\n",
    "   - \\( c = c0 - c1 \\)\n",
    "     - \\( c0 \\) is the clock reading before the step\n",
    "     - \\( c1 \\) is the clock reading after the step\n",
    "   - No clock tick \\( \\implies c = 0 \\)\n",
    "   - Clock tick \\( \\implies c < 0 \\)\n",
    "\n",
    "3. **d**: a death penalty that penalizes the agent for dying in a state\n",
    "   - This penalty encourages the agent to avoid death\n",
    "   - Alive \\( \\implies d = 0 \\)\n",
    "   - Dead \\( \\implies d = -15 \\)\n",
    "\n",
    "\\( r = v + c + d \\)\n",
    "\n",
    "The reward is clipped into the range \\([-15, 15]\\).\n",
    "\n",
    "In addition to the gym-super-mario-bros library, nes-py is required in order to emulate the NES in Python. Huge thanks to Kautenja for creating the environment and making this project possible! (Kauten, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aeec23-5c3b-4af6-a8c8-ee157900fc45",
   "metadata": {},
   "source": [
    "<a id='approach'></a>\n",
    "\n",
    "## Approach\n",
    "\n",
    "The initial setup for the environment was a bit tricky due to some incompatibilities between the chosen gym library gym-super-mario-bros JoypadSpace wrapper and the current version of OpenAi's gym framework, specifically with the `reset` method. Huge thanks to NathanGavinski who supplied [a workaround](https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091) in the issues forum for gym-super-mario-bros Git. [(NathanGavenski, 2023)](#references).\n",
    "\n",
    "The following code utilizes this fix along with the suggested boilerplate setup from the gym-super-mario-bros documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05dc07bf-8674-4287-878a-569fd9ab9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gymnasium.wrappers import StepAPICompatibility, TimeLimit\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2b346",
   "metadata": {},
   "source": [
    "<a id='random-agent'></a>\n",
    "\n",
    "## Random Agent\n",
    "\n",
    "Let's create an agent that makes random movements just to make sure our environment is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de8bc496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 594\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "done = True\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "# Run the environment for 5000 steps\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    done = done or truncated\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.05 seconds between frames\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27aa4a",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/random_agent.gif)\n",
    "\n",
    "You can see that the random agent never gets past the second pipe. This is because it is not probabilistically reasonable for random inputs to know to sustain a jump by pressing and holding A to get high enough to clear the pipe and keep going. This pipe exists at an X value of 595/596. Let's see if there are any other agents that can get farther."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a4f41",
   "metadata": {},
   "source": [
    "<a id='heuristic-agent'></a>\n",
    "\n",
    "## Heuristic Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489cad9-4b55-4b39-bf99-e12bab7547da",
   "metadata": {},
   "source": [
    "Although it is not reinforcement learning, I decided to implement a basic heuristic model that uses a simple algorithm to try to beat a level of Super Mario Bros which will serve as our milestone to beat. If our RL agent is able to surpass the heuristic agent, we will consider the RL agent to be a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7347e250-b94d-4483-8810-fdcfe114b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 199\n",
      "Max World: 1\n",
      "Max Stage: 2\n"
     ]
    }
   ],
   "source": [
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)\n",
    "\n",
    "# create global variables for inputs\n",
    "done = True\n",
    "going_up = False\n",
    "prev_y = None\n",
    "min_y = 0\n",
    "max_y = 0\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "for step in range(1700):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        prev_y = None\n",
    "        hold_jump = False\n",
    "    \n",
    "    # if Mario is on flat groun\n",
    "    # or in the process of rising from previous jump\n",
    "    # will continue to hold A to perform the maximum jump\n",
    "    action = SIMPLE_MOVEMENT.index(['right', 'A', 'B']) if going_up else SIMPLE_MOVEMENT.index(['right', 'B'])\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # set going_up to true if Mario is not descending\n",
    "    if prev_y is not None:\n",
    "        if info['y_pos'] >= prev_y:\n",
    "            going_up = True\n",
    "        else:\n",
    "            going_up = False\n",
    "\n",
    "    # capture current y position to compare for next state\n",
    "    prev_y = info['y_pos']\n",
    "        \n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "\n",
    "    if done or truncated:\n",
    "        done = True\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.01 seconds between frames to slow down render\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4640cca",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/heuristic_agent.gif)\n",
    "\n",
    "This heuristic, while not an actual learning model, is effective by sheer luck. This strategy is similar to what an actual player might try when attempting to beat the first level. It is intuitive to try and jump as high and as often as possible to clear most obstacles and enemies. It is by sheer luck (and some deaths) that the heuristic is able to avoid enemies and pits. Despite this, however, this agent is able to clear the first level, although it does not get very far in the second level.\n",
    "\n",
    "Let's try to create a reinforcement learning model and see if the agent can beat the level without relying on luck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4371",
   "metadata": {},
   "source": [
    "<a id='ppo-agent'></a>\n",
    "\n",
    "\n",
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5b078",
   "metadata": {},
   "source": [
    "\n",
    "Let's actually have an agent that learns, rather than blindly jumping. The initial model of choice will be a Proximal Policy Optimization (PPO) agent. This type of reinforcement learning model learns a policy that directly maps an approximation of the optimal action for any given state to exploit  to approximate the optimal action for any given state to maximize the reward returned over time.\n",
    "\n",
    "It does this by using gradient ascent to maximize the policy using the clipped objective function, which helps avoid making drastic weight updates:\n",
    "\n",
    "$L(\\theta) = \\mathbb{E} \\left[ \\min \\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}, \\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A} \\right) \\right]$\n",
    "\n",
    "This function seeks to optimize the parameters ($\\theta$) of the neural network by evaluating the expected value ($\\mathbb{E}$) of the minimum between the following two values:\n",
    "\n",
    "$$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}$$\n",
    "\n",
    "<p style=\"text-align: center;\">or</p>\n",
    "\n",
    "$$\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}$$\n",
    "\n",
    "$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}$ is the policy ratio, or how much the new policy $\\pi_\\theta$ has changed in relation to the old policy $\\pi_{old}$ and $\\hat A$ is the advantage, or how much better or worse the action $a$ is compared to the average action taken in state $s$\n",
    "\n",
    "The first term represents the weight change when the policy ratio is directly multiplied by the advantage. The second uses the clipping function to constrain the value of the policy ratio within the range $1 - \\epsilon$ and $1 + \\epsilon$ before multiplying with the advantage $\\hat A$\n",
    "\n",
    "Finding the expected value of the minimum of these two terms results in a gradient ascent that should prevent drastic policy updates and ensure more stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "\n",
    "import numpy as np\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "import gym # v0.26.2\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "from gym import Wrapper\n",
    "from gym import RewardWrapper\n",
    "import os\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        # create a reward accumulator\n",
    "        reward_accum = 0.0\n",
    "        done = False\n",
    "        for _ in range(self.skip):\n",
    "            next_state, reward, done, truncated, info = self.env.step(action)\n",
    "            # add reward to accumulator\n",
    "            reward_accum += reward\n",
    "            if done:\n",
    "                break\n",
    "        return next_state, reward_accum, done, truncated, info\n",
    "\n",
    "# final custom reward calculation after failed training (see below for details)\n",
    "class CustStepReward(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_y_pos = 0\n",
    "        self.reward_mean = 0\n",
    "        self.reward_var = 1\n",
    "        self.num_rewards = 0\n",
    "        self.succesive_A_presses = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        next_state, reward, done, truncated, info = self.env.step(action)\n",
    "        # if rising, agent must have pressed A state before\n",
    "        if self.prev_y_pos < info['y_pos']:\n",
    "            self.succesive_A_presses += 1\n",
    "            reward += 2 * self.succesive_A_presses # incentivize jumping higher if already jumping\n",
    "        # agent is not rising\n",
    "        else:\n",
    "            # reset to 0\n",
    "            # no reward (and no penalty)\n",
    "            self.succesive_A_presses = 0\n",
    "        self.prev_y_pos = info['y_pos']\n",
    "        \n",
    "        if info['life'] < 2:\n",
    "            reward -= 50 # heavily penalize death and end the episode\n",
    "            done = True\n",
    "        if info['flag_get']:\n",
    "            reward += 100 # heavily incentivize beating the level\n",
    "        \n",
    "        # Update reward statistics\n",
    "        self.num_rewards += 1\n",
    "        old_mean = self.reward_mean\n",
    "        self.reward_mean += (reward - self.reward_mean) / self.num_rewards\n",
    "        self.reward_var += (reward - old_mean) * (reward - self.reward_mean)\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward_std = np.sqrt(self.reward_var / self.num_rewards)\n",
    "        normalized_reward = (reward - self.reward_mean) / (reward_std + 1e-8)\n",
    "        return next_state, normalized_reward, done, truncated, info\n",
    "\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps\n",
    "\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, CUSTOM_ACTIONS)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "env = StepAPICompatibility(env, output_truncation_bool=True)\n",
    "env = CustStepReward(env)\n",
    "\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = ResizeObservation(env, shape=84) # reduce size of frame image\n",
    "env = GrayScaleObservation(env) # create grayscale images\n",
    "env = FrameStack(env, num_stack=8, lz4_compress=True) # stack frames\n",
    "\n",
    "model = PPO(\n",
    "    'CnnPolicy',      # Use a convolutional neural network\n",
    "    env,              # Game environment\n",
    "    verbose=1,        # print diagnostics\n",
    "    learning_rate=3e-4,  # how much to adjust the model with each step\n",
    "    n_steps=512,      # frequency of updates\n",
    "    batch_size=128,   # number of state samples evaluated in clipping objective function\n",
    "    clip_range = 0.2, # range to clip policy ratio in the clipping objective function\n",
    "    ent_coef = 0.9,   # entropy coefficient to encourage exploration\n",
    "    gamma = 0.99      # diminishes rewards for future action-state returns\n",
    ")\n",
    "\n",
    "\n",
    "# uncomment if continuing a previous training session\n",
    "#model.load(os.path.join(\"trained_agents\",\"ppo\",\"ppo_mario.zip\"))\n",
    "\n",
    "# Define evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=os.path.join('trained_agents','ppo'), log_path=os.path.join('trained_agents','ppo','logs'), eval_freq=5000, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=25000, save_path=os.path.join('trained_agents','ppo'), name_prefix='ppo_mario')\n",
    "\n",
    "model.learn(total_timesteps=700000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "model.save(os.path.join('trained_agents','ppo','ppo_mario_2'))\n",
    "\n",
    "# Evaluate the agent\n",
    "print(\"evaluating policy...\")\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "env.close()\n",
    "print(f\"Mean reward: {mean_reward} Â± {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64db28-02ae-4399-990e-9d92f365b646",
   "metadata": {},
   "source": [
    "To optimize the learning process, I decided to use several wrappers for optimization. The idea for these optimizations came from a video by Sourish Kundu I used as the basis for the Q-Learning model implemented later in this project [(Kundu, 2023)](#references). The following additions implemented above along with configuring Stable Baselines to use the GPU sped up the learning process quite a bit and allowed me to train the model for 400,000 episodes over the course of only a few hours.\n",
    "\n",
    "The first optimization is to create a custom wrapper that skips every four frames. Since the NES Mario Bros runs at about 60 fps, not much changes in the game state for each frame and the agent will choose an action and repeat the action for every four frames.\n",
    "\n",
    "Additional optimizations included downgrading from color RGB pixel values to gray scale values, reducing the image size, and stacking 8 frames at a time. The framestacking, when combined with the frame skipping, reduces the processing complexity of the model and at the same time allow it to interpret more meaningful game state changes than it would otherwise evaluate from every individual frame.\n",
    "\n",
    "The actual architecture of the model uses a convolutional neural network as seen from printing `model.policy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7565d176-b521-40ad-acc1-f00cbfd607d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "ActorCriticCnnPolicy(\n",
      "  (features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=46592, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (pi_features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=46592, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (vf_features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=46592, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential()\n",
      "    (value_net): Sequential()\n",
      "  )\n",
      "  (action_net): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, CUSTOM_ACTIONS)\n",
    "\n",
    "model = PPO(\n",
    "    'CnnPolicy',      # Use a convolutional neural network\n",
    "    env,              # Game environment\n",
    "    verbose=1,        # print diagnostics\n",
    "    learning_rate=3e-4,  # how much to adjust the model with each step\n",
    "    n_steps=512,      # frequency of updates\n",
    "    batch_size=128,   # number of state samples evaluated in clipping objective function\n",
    "    clip_range = 0.2, # range to clip policy ratio in the clipping objective function\n",
    "    ent_coef = 0.9,   # entropy coefficient to encourage exploration\n",
    "    gamma = 0.99      # diminishes rewards for future action-state returns\n",
    ")\n",
    "\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc768c2-9681-447a-8eea-117734caa1e3",
   "metadata": {},
   "source": [
    "The CNN architecture described is the NatureCNN, originally developed for Deep Q-Networks (DQN) to process high-dimensional visual inputs. It's used with the environment to get important spatial features from the game state using a combination of convolutional (Conv2d) layers and fully connected linear layers utilizing the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6daca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "\n",
    "import numpy as np\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "env = StepAPICompatibility(env, output_truncation_bool=True)\n",
    "#env = CustStepReward(env)\n",
    "\n",
    "#env = SkipFrame(env, skip=4)\n",
    "#env = ResizeObservation(env, shape=84) # reduce size of frame image\n",
    "#env = GrayScaleObservation(env) # create grayscale images\n",
    "#env = FrameStack(env, num_stack=8, lz4_compress=True) # stack frames\n",
    "\n",
    "# uncomment to load saved trained model\n",
    "# model = PPO.load(os.path.join(\"trained_agents\",\"ppo\",\"ppo_mario_25000_steps.zip\"))\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "obs, info = env.reset()\n",
    "for step in range(2000):\n",
    "    action, info = model.predict(np.array(obs))\n",
    "    action = action.item()       \n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4def3",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/initial_ppo.gif)\n",
    "\n",
    "With the default PPO model values, the agent is unable to clear the pipe at the 595 X coordinate. I allowed the agent to train for 100,000 episodes hoping it would figure out that keeping the A button held down would lead to a higher jump, allowing it to continue to the right. Unfortunately, after 100,000 iterations, the model instead runs directly into the first goomba over and over.\n",
    "\n",
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/goomba_death.gif)\n",
    "\n",
    "I attempted to fine tune the learning rate using a scheduler, fine tuning the gamma and clip values, as well as included an entropy coefficient to encourage exploration to no avail. Even with a large entropy coefficient, the model was not able to get over the pipe by chance. In retrospect, this is consistent with the behavior with our random agent, as the chances of stringing together several A button presses while also holding right for forward momentum could be rather unlikely. Even with the addition of frame skipping, which causes the agent to perform an action for every four frames, the agent was unable to clear the pipe in any of the learning episodes.\n",
    "\n",
    "__Because of this, the agent always gets stuck at either the first or second pipe and accrues penalties for the clock ticking down with no forward progress, per the default reward structure. After 100,000 episodes, the agent seems to learn that in order to avoid this penalty, it is more optimal to run directly into the first goomba, receiving a smaller penalty from taking a death than the accrued penalty it receives from getting stuck at the pipe.__\n",
    "\n",
    "To counteract this, I added a new Gym environment wrapper that returns a new reward function. Learning from the heuristic agent, I believe Mario will both progress farther and avoid death more if he spends more time jumping. Because of this, I added a reward if Mario's change in Y-value is positive (meaning he is ascending). This should hopefully incentivize the agent to continuously press the A button and increase the chances that it is able to clear any tall pipes.\n",
    "\n",
    "Additionally, I chose to give a heavy penalty and terminate the episode when the agent dies, which should discourage the agent from exploiting any rewards it receives from taking an intentional death.\n",
    "\n",
    "Lastly, I added an incentive for the agent if the 'get_flag' bool is true, further incentivizing the agent to complete the level.\n",
    "\n",
    "Lastly, I normalized the returned reward based on the average reward of the episode in order to stabilize the learning process while I made adjustments:\n",
    "\n",
    "```python\n",
    "class CustStepReward(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_y_pos = 0\n",
    "        self.reward_mean = 0\n",
    "        self.reward_var = 1\n",
    "        self.num_rewards = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        next_state, reward, done, truncated, info = self.env.step(action)\n",
    "        # create custom reward value\n",
    "        if self.prev_y_pos < info['y_pos']:\n",
    "            reward += 4 # incentivize being airborn\n",
    "        self.prev_y_pos = info['y_pos']\n",
    "        if info['life'] < 2:\n",
    "            reward -= 50 # heavily penalize death and end the episode\n",
    "            done = True\n",
    "        if info['flag_get']:\n",
    "            reward += 100 # heavily incentivize beating the level\n",
    "        \n",
    "        # Update reward statistics\n",
    "        self.num_rewards += 1\n",
    "        old_mean = self.reward_mean\n",
    "        self.reward_mean += (reward - self.reward_mean) / self.num_rewards\n",
    "        self.reward_var += (reward - old_mean) * (reward - self.reward_mean)\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward_std = np.sqrt(self.reward_var / self.num_rewards)\n",
    "        normalized_reward = (reward - self.reward_mean) / (reward_std + 1e-8)\n",
    "        return next_state, normalized_reward, done, truncated, info\n",
    "```\n",
    "\n",
    "The end result worked as expected, and the agent was able to get farther in the level after training for 100,000 episodes. __However, after training for 400,000 episodes, the model again resorted to exploitation, this time using the reward for increasing y position. The reward it received for this outweighed the penalty for staying still and it found that the optimal policy was to stay in one place and jump continuously over and over to accrue rewards until the time ran out.__\n",
    "\n",
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/left_model.gif)\n",
    "\n",
    "To fix this, I revised the reward function again to only reward increasing y position if already in a jumping state and scale this reward by the number of successive A presses. This should hopefully encourage the agent to jump higher to gain a higher reward:\n",
    "\n",
    "```python\n",
    "if self.prev_y_pos < info['y_pos']:\n",
    "    self.succesive_A_presses += 1\n",
    "    reward += 2 * self.succesive_A_presses # incentivize jumping higher if already jumping\n",
    "# agent is not rising\n",
    "else:\n",
    "    # reset to 0\n",
    "    # no reward (and no penalty)\n",
    "    self.succesive_A_presses = 0\n",
    "self.prev_y_pos = info['y_pos']\n",
    "```\n",
    "\n",
    "Additionally, I simplified the movement actions to only `Right + B` for running right, and `Right + B + A` for a running jump.\n",
    "\n",
    "```python\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "```\n",
    "\n",
    "While this simplification may make it harder for the agent to avoid enemies, it is more in alignment with the heuristic agent we used as a baseline and should hopefully allow the agent to make better progress.\n",
    "\n",
    "With these revisions, the agent was able to complete the level and even get a bit farther after 600,000 training steps, without taking any deaths (per the harsh penalty implemented in the revised reward function).\n",
    "\n",
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/ppo_final.gif)\n",
    "\n",
    "This reward function seems to work as intended and the model learns with quite a bit of stability. Given enough iterations, I believe the model would continue to improve and progress farther into the game.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3c39e",
   "metadata": {},
   "source": [
    "<a id='q-learning-agent'></a>\n",
    "\n",
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c92186",
   "metadata": {},
   "source": [
    "To compare with the Stable Baseling3 PPO agent, I also create a Q-Learning agent. As referenced before, a huge thank you to Sourish Kundu, who made a video on how to do exactly this with some great explanations [(Kundu, 2023)](#references). If you have a chance, I highly recommend giving it a watch, even if you are not working with this gym environment, as it gives some great explanations of general concepts we have been discussing in class.\n",
    "\n",
    "https://www.youtube.com/watch?v=_gmQZToTMac\n",
    "\n",
    "https://github.com/Sourish07/Super-Mario-Bros-RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe369c",
   "metadata": {},
   "source": [
    "We can begin by creating the same wrappers as before. Initially, I used these wrappers for the Q-Learning model and applied them to the PPO model after the fact. We again skip 4 frames, rescale and reduce the images to gray scale while stacking 4 frames. I am also including the previous reward wrapper used in the PPO model before applying all these wrappers together in the function `apply_wrappers` that returns the environment utilizing all of the environment wrappers.\n",
    "\n",
    "One note is that the following algorithm requires gym version 0.24.0 as the returns of the reset function need to be a single value, as opposed to the tuple returned in later versions. Overriding the reset function does not work, as several of the necessary wrappers and Python modules are not compatible with the newer versions of gym. If you are attempting to run this cell, you will receive an error unless you run `pip install gym==0.24.0` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym import Wrapper\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        # create a reward accumulator\n",
    "        reward_accum = 0.0\n",
    "        done = False\n",
    "        for _ in range(self.skip):\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            # add reward to accumulator\n",
    "            reward_accum += reward\n",
    "            if done:\n",
    "                break\n",
    "        return next_state, reward_accum, done, info\n",
    "    \n",
    "class CustStepReward(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_y_pos = 0\n",
    "        self.reward_mean = 0\n",
    "        self.reward_var = 1\n",
    "        self.num_rewards = 0\n",
    "        self.succesive_A_presses = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # if rising, agent must have pressed A state before\n",
    "        if self.prev_y_pos < info['y_pos']:\n",
    "            self.succesive_A_presses += 1\n",
    "            reward += 2 * self.succesive_A_presses # incentivize jumping higher if already jumping\n",
    "        # agent is not rising\n",
    "        else:\n",
    "            # reset to 0\n",
    "            # no reward (and no penalty)\n",
    "            self.succesive_A_presses = 0\n",
    "        self.prev_y_pos = info['y_pos']\n",
    "        \n",
    "        if info['life'] < 2:\n",
    "            reward -= 50 # heavily penalize death and end the episode\n",
    "            done = True\n",
    "        if info['flag_get']:\n",
    "            reward += 100 # heavily incentivize beating the level\n",
    "        \n",
    "        # Update reward statistics\n",
    "        self.num_rewards += 1\n",
    "        old_mean = self.reward_mean\n",
    "        self.reward_mean += (reward - self.reward_mean) / self.num_rewards\n",
    "        self.reward_var += (reward - old_mean) * (reward - self.reward_mean)\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward_std = np.sqrt(self.reward_var / self.num_rewards)\n",
    "        normalized_reward = float((reward - self.reward_mean) / (reward_std + 1e-8))\n",
    "        return next_state, normalized_reward, done, info\n",
    "\n",
    "def apply_wrappers(env):\n",
    "    env = SkipFrame(env, skip=4) # skip every four frames\n",
    "    env = ResizeObservation(env, shape=84) # reduce size of frame image\n",
    "    env = GrayScaleObservation(env) # create grayscale images\n",
    "    env = FrameStack(env, num_stack=4, lz4_compress=True) # stack frames (4 skipped)\n",
    "    env = CustStepReward(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617c80e",
   "metadata": {},
   "source": [
    "Instead of using a default neural network, we will create our own using convolution layers and linear layers. We are using Conv2d layers with Relu activation functions, as these are typically appropriate for images (as I learned from HW5). Using MaxPool2d will reduce spatial dimensions and lower computational cost, which will hopefully lower the processing time for our algorithm.\n",
    "\n",
    "Additional methods are used to dynamically evaluate the input shape for our initial linear layer (`_get_conv_out`), prevent pytorch from updating the gradients if frozen (`_freeze`), and tell pytorch how to handle the forward pass for each tensor (`forward`).\n",
    "\n",
    "I reduced the complexity of the architecture from Sourish's video to improve processing speed, but much of the structure of the attributes and methods were influenced heavily by his example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48be6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class AgentNN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, freeze=False):\n",
    "        super().__init__()\n",
    "        # Conolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # use built-in method to get the dimensional input size for initial linear layer\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        # Fully connected linear layers\n",
    "        self.network = nn.Sequential(\n",
    "            self.conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions) # determine best action to predict\n",
    "        )\n",
    "\n",
    "        # call the freeze method if frozen\n",
    "        # to make sure no parameters are updated if frozen\n",
    "        if freeze:\n",
    "            self._freeze()\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # try to use the GPU if possible\n",
    "        self.to(self.device)\n",
    "\n",
    "    # method to handle forward pass \n",
    "    def forward(self, x):\n",
    "        # pass the input tensor through the neural network layers\n",
    "        return self.network(x)\n",
    "\n",
    "    # get the number of neurons for our linear layers\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv_layers(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    # method to make sure gradients are not calculated if frozen\n",
    "    def _freeze(self):        \n",
    "        for p in self.network.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f3f35",
   "metadata": {},
   "source": [
    "Next, we will create our agent class to use the neural network created previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32430615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tensordict import TensorDict # use tensors in python lists to speed up processing\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 input_dims, \n",
    "                 num_actions, \n",
    "                 lr=0.00025, \n",
    "                 gamma=0.9, \n",
    "                 epsilon=1.0, \n",
    "                 eps_decay=0.99999975, \n",
    "                 eps_min=0.1, \n",
    "                 replay_buffer_capacity=150000, \n",
    "                 batch_size=32, \n",
    "                 sync_network_rate=10000\n",
    "                 ):\n",
    "        \n",
    "        self.num_actions = num_actions # use the appropriate number of actions (SIMPLE_MOVEMENT dict has 7)\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_network_rate = sync_network_rate\n",
    "\n",
    "        # Networks\n",
    "        self.online_network = AgentNN(input_dims, num_actions)\n",
    "        self.target_network = AgentNN(input_dims, num_actions, freeze=True)\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.MSELoss() # loss function\n",
    "\n",
    "        # Replay buffer\n",
    "        storage = LazyMemmapStorage(replay_buffer_capacity)\n",
    "        self.replay_buffer = TensorDictReplayBuffer(storage=storage)\n",
    "        self.log_memory_usage()\n",
    "\n",
    "    def log_memory_usage(self):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_info = process.memory_info()\n",
    "        print(f\"Memory Usage: {mem_info.rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # create the potential to choose a random action\n",
    "        # this will include some value of randomness to increase exploration\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        \n",
    "        observation = (\n",
    "            torch.tensor(np.array(observation), dtype=torch.float32) # speed up processing by using tensors instead of numpy arrays\n",
    "            .unsqueeze(0) # add dimension of batch size to first index of tensor\n",
    "            .to(self.online_network.device) # move to the correct device (GPU or CPU)\n",
    "        )\n",
    "        # return the action with the highest Q-value\n",
    "        return self.online_network(observation).argmax().item()\n",
    "    \n",
    "    # compute the value of epsilon to diminish rewards for later actions\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min)\n",
    "\n",
    "    # put tensors in a dict and add to buffer\n",
    "    def store_in_memory(self, state, action, reward, next_state, done):\n",
    "        # Create TensorDict with correct shapes and types\n",
    "        data = TensorDict({\n",
    "            \"state\": torch.tensor(np.array(state), dtype=torch.float32),\n",
    "            \"action\": torch.tensor(action),\n",
    "            \"reward\": torch.tensor(reward),\n",
    "            \"next_state\": torch.tensor(np.array(next_state), dtype=torch.float32),\n",
    "            \"done\": torch.tensor(done)\n",
    "        }, batch_size=[])\n",
    "        self.replay_buffer.add(data)\n",
    "    \n",
    "    # copy weights of online network to target network if enough steps have passed\n",
    "    def sync_networks(self):\n",
    "        if self.learn_step_counter % self.sync_network_rate == 0 and self.learn_step_counter > 0:\n",
    "            self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "\n",
    "    # save current model (in case something goes wrong)\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.online_network.state_dict(), path)\n",
    "\n",
    "    # load model\n",
    "    def load_model(self, path):\n",
    "        self.online_network.load_state_dict(torch.load(path))\n",
    "        self.target_network.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        # if not enough experiences, return and keep going\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # copy weights to target network\n",
    "        self.sync_networks()\n",
    "        \n",
    "        # clear gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sample the replay buffer and store the results\n",
    "        samples = self.replay_buffer.sample(self.batch_size).to(self.online_network.device)\n",
    "        states = samples['state']\n",
    "        actions = samples['action']\n",
    "        rewards = samples['reward']\n",
    "        next_states = samples['next_state']\n",
    "        dones = samples['done']\n",
    "\n",
    "        # get the predicted values from our neural network with the appropriate batch size\n",
    "        predicted_q_values = self.online_network(states)\n",
    "        predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
    "\n",
    "        # Max returns two tensors, the first one is the maximum value, the second one is the index of the maximum value\n",
    "        target_q_values = self.target_network(next_states).max(dim=1)[0]\n",
    "        # The rewards of any future states don't matter if the current state is a terminal state\n",
    "        # If done is true, then 1 - done is 0, so the part after the plus sign (representing the future rewards) is 0\n",
    "        target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
    "\n",
    "        loss = self.loss(predicted_q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "        self.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058d5e2",
   "metadata": {},
   "source": [
    "This class performs an approximate mathematical implementation of the Bellman Equation for calculating Q-values for each state-action pair:\n",
    "\n",
    "$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$\n",
    "\n",
    "As seen here:\n",
    "\n",
    "```python\n",
    "target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
    "```\n",
    "\n",
    "PyTorch's built-in Mean Squared Error loss function is applied between predicted and target Q-values:\n",
    "```python\n",
    "self.losss = torch.nn.MSE()\n",
    "```\n",
    "```python\n",
    "loss = self.loss(predicted_q_values, target_q_values)\n",
    "```\n",
    "\n",
    "This is then minimized using backpropagation in\n",
    "```python\n",
    "loss.backward(); self.optimizer.step() in learn())\n",
    "```\n",
    "\n",
    "The agent selects actions using an epsilon-greedy policy  as seen in the `choose_action()` function, where the epsilon value represents the probability the model chooses a random action, selecting the action with the highest Q-value the rest of the time. This epsilon value decays overtime as seen here:\n",
    "\n",
    "```python\n",
    "self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min\n",
    "```\n",
    "\n",
    "To stabilize learning, the agent periodically synchronizes the target network with the online network `self.sync_networks()`, copying the weights from the online network to the target network after a certain number of steps.\n",
    "\n",
    "We now have everything we need to create the environment and run our reinforcement learning architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7554543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
    "SAVE_INTERVAL = 1000\n",
    "DISPLAY = True\n",
    "NUM_OF_EPISODES = 100000\n",
    "\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right','A','B'],\n",
    "    ['right','B']\n",
    "]\n",
    "\n",
    "env = gym_super_mario_bros.make(ENV_NAME)\n",
    "env = JoypadSpace(env, CUSTOM_ACTIONS)\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "\n",
    "# create a progress bar when training\n",
    "def print_progress(cur,end, bar_length=40):\n",
    "    progress = cur / end\n",
    "    block = int(round(bar_length * progress))\n",
    "    text = f\"\\rCurrently processing episode {cur}/{end} [{'#' * block + '-' * (bar_length - block)}] {progress * 100:.2f}%\"\n",
    "    print(text, end='', flush=True)\n",
    "\n",
    "for i in range(NUM_OF_EPISODES):\n",
    "    print_progress(i+1,NUM_OF_EPISODES)\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        a = agent.choose_action(state)\n",
    "        new_state, reward, done, info  = env.step(a)\n",
    "        \n",
    "        agent.store_in_memory(state, a, reward, new_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        # save the model every save interval\n",
    "        if (i + 1) % SAVE_INTERVAL == 0:\n",
    "            agent.save_model(os.path.join(\"trained_agents\\q_learning\",str(i + 1) + \"_q_agent.pt\"))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3a1bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 436.52 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sethe\\AppData\\Local\\Temp\\ipykernel_20728\\3851061806.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.online_network.load_state_dict(torch.load(path))\n",
      "C:\\Users\\sethe\\AppData\\Local\\Temp\\ipykernel_20728\\3851061806.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.target_network.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 1967\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
    "SHOULD_TRAIN = True\n",
    "DISPLAY = True\n",
    "NUM_OF_EPISODES = 5\n",
    "\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "\n",
    "env = gym_super_mario_bros.make(ENV_NAME)\n",
    "env = JoypadSpace(env, CUSTOM_ACTIONS)\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "\n",
    "agent.load_model(os.path.join(\"trained_agents\",\"q_learning\",\"27000_q_agent.pt\"))\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "state = env.reset()\n",
    "for step in range(5000):\n",
    "    action = agent.choose_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7fa66",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/q_27000.gif)\n",
    "\n",
    "WIth only 27,000 training episodes, the Q Learning algorithm is able to take advantage of the height reward and is clearing the tall pipes and the first gap. Given enough iterations, I believe this algorithm would show similar, if not better results than our PPO algorithm. However, it took almost 24 hours to just get this number of iterations when I could get 20 times the number of episodes with the PPO algorithm.\n",
    "\n",
    "Since the main difference between PPO and Q-Learning, besides the processing time, has to do with how they handle discrete/continuous action spaces, limiting the number of chosen actions may have improved the Q-Learning algorithm. Even though the game-state is continuous and the controls for Mario can be very complex and nuanced, the limitation of either Running right (right + B) or jumping right (right + B + A) in addition to the frame skipping would have hopefully simplified the gamestate to be more similar to a discrete action state and likely improved the performance of the model.\n",
    "\n",
    "Additionally, initial PPO algorithm was more prone to exploitation, which caused me to fine-tune the environment's reward value function and incorporate an entropy coefficient to encourage the agent to explore. In the same way, Q-Learning uses an epsilon greedy function that chooses a random action some of the time, taking the the action with the highest q-value the rest of the time. Using the more balanced reward function and set of actions taken from tuning the PPO algorithm assisted in stabilizing the Q-Learning model without as much trial and error. This was significant, especially given the the costly time to process the Q-Learning model as opposed to the PPO model.\n",
    "\n",
    "In contrast, optimizations I found for the Q-Learning model to decrease computational complexity, such as frame skipping/stacking, image size reduction, grayscaling and not to mention configuring the modules to use the GPU, were able to be applied to the PPO model, making it even faster and allowing me to run even more iterations to easily fine tune the parameters and check the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28095c2-22be-4762-812f-d5210cd89f2b",
   "metadata": {},
   "source": [
    "<a id='results'></a>\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28409ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Model     | Video of Model Predictions  | Notes    |Max World       | Max Level | Max X Position | Number of Training Steps  |\n",
    "|------------------|---- |---------------|----------------|-----------|-----------|-----------|\n",
    "| __Random Agent__     |  ![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/random_agent.gif)   |    Environment is working, but agent cannot progress far with random actions.        | 1            | 1         | 595         | N/A | \n",
    "| __Heuristic Agent__  | ![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/heuristic_agent.gif)   |  Agent wins using strategy and luck (not reinforcement learning)         | 1            | 2         | 199         | N/A |\n",
    "| __PPO Agent__        |  ![image](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/ppo_final.gif)   | Refining reward function keeps the agent from exploiting default rewards with unintended behavior        | 1            | 2         | 482         | 600,000 (processed in 6 hours) |\n",
    "| __Q Learning Agent__ | ![images](https://raw.githubusercontent.com/seel6470/CSPB-3202-Final-Project/main/images/q_27000.gif)   | Model is stable, but processing is very slow         | 1            | 1         | 1967         | 27,000 (Processed in 24 hours) |\n",
    "\n",
    "In summary, it is worth noting that the PPO agent made it the farthest. Althought the heuristic agent beat the first level, it did so without any reinforcement learning and only after taking a few deaths. Additionally, the Q-Learning agent made it particularly far in only 27,000 training episodes. Given enough iterations, I believe this agent would out-perform the PPO agent, although the processing time is a great hinderance to this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455bdd-f926-4764-83ef-dbd6fc961590",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e061287",
   "metadata": {},
   "source": [
    "In the end the heuristic created a strong strategy for simplifying the model's action choices, and creating a set of custom actions to either run right, or run and jump right encouraged the model to control Mario similar to a real player, jumping over most obstacles and minimizing the chances for death by running into enemies on the ground.\n",
    "\n",
    "Utilizing the custom reward function to encourage higher jumping and creating the simplified action list allowed both the PPO and Q-Learning agents to progress much farther than initially, and ensured stable learning that balanced both exploration and exploitation. Prior to these revisions, the model continuously exploited the reward mechanics to a point that it was not progressing further into the game and would eventually converge on a policy that either stayed in one position (either stuck at a pipe or jumping over and over) or killing itself to the firts goomba repeatedly.\n",
    "\n",
    "The main difference in performance between the Q-Learning and PPO models was mainly down to processing complexity. The PPO model was simpler and was able to perform hundreds of thousands of training episodes in only a few hours, whereas the Q-Learning algorithm, while it was significantly more stable, took over 24 hours just to get to 50,000 training episodes. The benefits of the processing speed of Stable Baseline3's PPO library enabled me to see progress quickly and make adjustments as needed, whereas it was relatively difficult to figure out any problems with the Q-Learning algorithm until a considerable amount of time was wasted.\n",
    "\n",
    "Because of the processing drawbacks, I was forced to consider further optimizations to improve performance which I could use to speed up the SB3 PPO model even more.\n",
    "\n",
    "At the same time, I was able to implement the improvements made to the reward and action choices of the PPO model to further improve the Q-Learning model. Insights which would have taken days and weeks to gather if I simply used the Q-Learning agent alone.\n",
    "\n",
    "It would be interesting to see how much the Q-Learning agent would improve given the same number of iterations the PPO model was able to make and compare them side by side. Additionally, I have seen other projects, like [this one by William Wei](https://github.com/yumouwei/super-mario-bros-reinforcement-learning), that use custom wrappers that are able to read and interpret the memory of the Super Mario Bros environment to be able to have even more information for the agent to utilize, determining the locations of enemies, blocks, pits, etc [(Yumouwei, 2023)](#references).\n",
    "\n",
    "Given additional time, I would like to see how well the agent would be able to improve with this information. This in combination with using random stage selection could also allow the model to be able to create a more generalized optimal policy that could assist in beating many levels of the game. As it is, my model seems to be making progress very slowly as it learns from trial and error as opposed to dynamically \"seeing\" information in the game state and choosing actions accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6817d-c989-41d6-9a06-2c128b50ae0a",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "\n",
    "## References\n",
    "\n",
    "Gavenski, Nathan. (2023). Comment on issue #128 in Kautenja/gym-super-mario-bros repository. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091\n",
    "\n",
    "Kauten, Christian. (2018). Super Mario Bros for OpenAI Gym. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros\n",
    "\n",
    "Kundu, Sourish. (2023). Super Mario Bros Reinforcement Learning. GitHub, 2023, https://github.com/Sourish07/Super-Mario-Bros-RL.\n",
    "\n",
    "Kundu, Sourish. (2023). Train AI to beat Super Mario Bros! || Reinforcement learning completely from scratch [Video]. YouTube. https://www.youtube.com/watch?v=_gmQZToTMac\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. [Video game]. Nintendo. \n",
    "\n",
    "Wei, William. (2023). Super Mario Bros Reinforcement Learning [Source code]. GitHub. https://github.com/yumouwei/super-mario-bros-reinforcement-learning\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

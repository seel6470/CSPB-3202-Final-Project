{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffc2037-5366-430d-bb52-2170e163d598",
   "metadata": {},
   "source": [
    "GitHub Repository for this project can be found at https://github.com/seel6470/CSPB-3202-Final-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8866730-fe6d-4e2f-ba9d-61204c3f1aa0",
   "metadata": {},
   "source": [
    "## Video Clip of Finished Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99cf0f-1b67-44ab-a413-64bd96974842",
   "metadata": {},
   "source": [
    "### Short Overview\n",
    "\n",
    "*short overview of what your project is about (e.g. you're building /testing certain RL models in certain environments; yes you can test your algorithm in more than 1 environment if your goal is to test an algorithm(s) performances in different settings)*\n",
    "\n",
    "1. Does it include the clear overview on what the project is about? (4)\n",
    "\n",
    "2. Does it explain how the environment works and what the game rules are? (4)\n",
    "\n",
    "For my project, I chose to teach a learning model to play the original Super Mario Bros. game for the NES. I utilized a library created by Christian Kauten called gym-super-mario-bros, which provides an OpenAI Gym environment using the nes-py emulator (Kauten, 2018). The challenge is to beat as many levels as possible in the original Mario game for NES with the following rules of the game.\n",
    "\n",
    "The goal of the game is to avoid enemies and pits to reach the end of each level. One hit and Mario loses a life, starting over from the nearest checkpoint. Power-ups provide Mario an additional hit. The following page from the original game manual outlines the inputs Mario receives for the game:\n",
    "\n",
    "![image](images/controls.jpg)\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "The game environment takes these controls and creates the following action lists that can be used within the environment wrapper:\n",
    "\n",
    "```python\n",
    "# actions for the simple run right environment\n",
    "RIGHT_ONLY = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for very simple movement\n",
    "SIMPLE_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for more complex movement\n",
    "COMPLEX_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "    ['left', 'A'],\n",
    "    ['left', 'B'],\n",
    "    ['left', 'A', 'B'],\n",
    "    ['down'],\n",
    "    ['up'],\n",
    "]\n",
    "```\n",
    "\n",
    "The environment can also determine the following keys for the gamestate:\n",
    "\n",
    "| Key       | Type | Description                                |\n",
    "|-----------|------|--------------------------------------------|\n",
    "| coins     | int  | The number of collected coins              |\n",
    "| flag_get  | bool | True if Mario reached a flag or ax         |\n",
    "| life      | int  | The number of lives left, i.e., {3, 2, 1}  |\n",
    "| score     | int  | The cumulative in-game score               |\n",
    "| stage     | int  | The current stage, i.e., {1, ..., 4}       |\n",
    "| status    | str  | Mario's status, i.e., {'small', 'tall', 'fireball'} |\n",
    "| time      | int  | The time left on the clock                 |\n",
    "| world     | int  | The current world, i.e., {1, ..., 8}       |\n",
    "| x_pos     | int  | Mario's x position in the stage (from the left) |\n",
    "| y_pos     | int  | Mario's y position in the stage (from the bottom) |\n",
    "\n",
    "Additionally, the environment utilizes the following parameters for the reward function:\n",
    "\n",
    "v: the difference in agent x values between states\n",
    "\n",
    "c: the difference in the game clock between frames\n",
    "\n",
    "d: a death penalty that penalizes the agent for dying in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aeec23-5c3b-4af6-a8c8-ee157900fc45",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "*explain your environment, your choice of model(s), the methods and purpose of testing and experiments, explain any trouble shooting required.*\n",
    "\n",
    "3. Does it explain clearly the model(s) of choices, the methods and purpose of tests and experiments? (7)\n",
    "\n",
    "4. Does it show problem solving procedure- e.g. how the author solved and improved when an algorithm doesn't work well. Note that it's not about debugging or programming/implementation, but about when a correctly implemented algorithm wasn't enough for the problem and the author had to modify/add some features or techniques, or compare with another model, etc. (7)\n",
    "\n",
    "The initial setup for the environment was a bit tricky due to some incompatibilities between the chosen gym library gym-super-mario-bros JoypadSpace wrapper and the current version of OpenAi's gym framework, specifically with the `reset` method. Huge thanks to NathanGavinski who supplied [a workaround](https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091) in the issues forum for gym-super-mario-bros Git. (NathanGavenski, 2023).\n",
    "\n",
    "The following code utilizes this fix along with the suggested boilerplate setup from the gym-super-mario-bros documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05dc07bf-8674-4287-878a-569fd9ab9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gymnasium.wrappers import StepAPICompatibility, TimeLimit\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489cad9-4b55-4b39-bf99-e12bab7547da",
   "metadata": {},
   "source": [
    "To get a baseling, I decided to implement a basic heuristic model that uses a simple algorithm to try to beat a level of Super Mario Bros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7347e250-b94d-4483-8810-fdcfe114b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# if Mario is on flat groun\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# or in the process of rising from previous jump\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# will continue to hold A to perform the maximum jump\u001b[39;00m\n\u001b[0;32m     15\u001b[0m action \u001b[38;5;241m=\u001b[39m SIMPLE_MOVEMENT\u001b[38;5;241m.\u001b[39mindex([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m going_up \u001b[38;5;28;01melse\u001b[39;00m SIMPLE_MOVEMENT\u001b[38;5;241m.\u001b[39mindex([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 16\u001b[0m state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# set going_up to true if Mario is not descending\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\step_api_compatibility.py:53\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, returning 5 or 4 items depending on `output_truncation_bool`.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     step_returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_api_compatibility(\n\u001b[0;32m     55\u001b[0m         step_returns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_truncation_bool, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env\n\u001b[0;32m     56\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nes_py\\nes_env.py:300\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[0;32m    302\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create global variables for inputs\n",
    "done = True\n",
    "going_up = False\n",
    "prev_y = None\n",
    "\n",
    "for step in range(1700):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        prev_y = None\n",
    "        hold_jump = False\n",
    "    \n",
    "    # if Mario is on flat groun\n",
    "    # or in the process of rising from previous jump\n",
    "    # will continue to hold A to perform the maximum jump\n",
    "    action = SIMPLE_MOVEMENT.index(['right', 'A', 'B']) if going_up else SIMPLE_MOVEMENT.index(['right', 'B'])\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # set going_up to true if Mario is not descending\n",
    "    if prev_y is not None:\n",
    "        if info['y_pos'] >= prev_y:\n",
    "            going_up = True\n",
    "        else:\n",
    "            going_up = False\n",
    "\n",
    "    # capture current y position to compare for next state\n",
    "    prev_y = info['y_pos']\n",
    "        \n",
    "    if done or truncated:\n",
    "        done = True\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.01 seconds between frames\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "482d8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.previous_y_pos = 0  # Initialize the previous y position\n",
    "\n",
    "    def reward(self, reward):\n",
    "        # Access the private attribute _y_position\n",
    "        current_y_pos = self.unwrapped._y_position\n",
    "\n",
    "        # Calculate the change in y position\n",
    "        delta_y = current_y_pos - self.previous_y_pos\n",
    "\n",
    "        # Update the previous y position\n",
    "        self.previous_y_pos = current_y_pos\n",
    "\n",
    "        reward += 0.2 * delta_y\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9957fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 120 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 1   |\n",
      "|    total_timesteps | 128 |\n",
      "----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 30         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 256        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03974242 |\n",
      "|    clip_fraction        | 0.375      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.677     |\n",
      "|    explained_variance   | 0.00109    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 120        |\n",
      "|    n_updates            | 4          |\n",
      "|    policy_gradient_loss | -0.00242   |\n",
      "|    value_loss           | 451        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 384         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000563025 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0799      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 145         |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 947         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 512        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01759155 |\n",
      "|    clip_fraction        | 0.0527     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.653     |\n",
      "|    explained_variance   | 0.0276     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 391        |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | -0.00357   |\n",
      "|    value_loss           | 422        |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 21            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 30            |\n",
      "|    total_timesteps      | 640           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028464152 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.602        |\n",
      "|    explained_variance   | 0.202         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 68.3          |\n",
      "|    n_updates            | 16            |\n",
      "|    policy_gradient_loss | 0.00151       |\n",
      "|    value_loss           | 161           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 768          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010639504 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.634       |\n",
      "|    explained_variance   | 0.0141       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 8.44         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.00579      |\n",
      "|    value_loss           | 163          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 20          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 896         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020563496 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.00991     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 41.7        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0018     |\n",
      "|    value_loss           | 72.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 19           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043742484 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 0.0281       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | 0.000182     |\n",
      "|    value_loss           | 68.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x217998a8ef0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "class CustomRewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "        self.previous_y_pos = 0  # Initialize the previous y position\n",
    "\n",
    "    def reward(self, reward):\n",
    "        # Access the private attribute _y_position\n",
    "        current_y_pos = self.unwrapped._y_position\n",
    "\n",
    "        # Calculate the change in y position\n",
    "        delta_y = current_y_pos - self.previous_y_pos\n",
    "\n",
    "        # Update the previous y position\n",
    "        self.previous_y_pos = current_y_pos\n",
    "\n",
    "        reward += 0.2 * delta_y\n",
    "\n",
    "        return reward\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B']\n",
    "]\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, CUSTOM_ACTIONS)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)\n",
    "env = CustomRewardWrapper(env)\n",
    "# Create and train the PPO agent\n",
    "model = PPO('CnnPolicy', env, verbose=1, learning_rate=1e-4, n_steps=128, batch_size=64, n_epochs=4, clip_range=0.2)\n",
    "\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "#model.save(\"ppo_mario\")\n",
    "\n",
    "# Evaluate the agent\n",
    "#mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "#print(f\"Mean reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c6daca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "for step in range(1500):\n",
    "    action, _states = model.predict(obs.copy())\n",
    "    action = action.item()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28095c2-22be-4762-812f-d5210cd89f2b",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "*show the result and interpretation of your experiment. Any iterative improvements summary.*\n",
    "\n",
    "5. Does it include the results summary, interpretation of experiments and visualization (e.g. performance comparison table, graphs etc)? (7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455bdd-f926-4764-83ef-dbd6fc961590",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "*Conclusion, discussion, reflection, or suggestions for future improvements or future ideas.*\n",
    "\n",
    "6. Does it include discussion (what went well or not and why), and suggestions for improvements or future work? (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6817d-c989-41d6-9a06-2c128b50ae0a",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "*Reference: Please include all relevant links (git, video, etc)*\n",
    "\n",
    "7. Does it include all deliverables (3)\n",
    "\t- git with codes or notebooks\n",
    "\t- writeup (you can consider notebook as a writeup if the notebook contains all needed contents and explanation)\n",
    "\t- demo clips\n",
    "\t- proper quote or reference\n",
    "    \n",
    "Kauten, C. (2018). Super Mario Bros for OpenAI Gym. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "NathanGavenski. (2023). Comment on issue #128 in Kautenja/gym-super-mario-bros repository. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b94f9b-15f1-48eb-8b0d-8584b7a585a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

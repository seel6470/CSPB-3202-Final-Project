{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffc2037-5366-430d-bb52-2170e163d598",
   "metadata": {},
   "source": [
    "GitHub Repository for this project can be found at https://github.com/seel6470/CSPB-3202-Final-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8866730-fe6d-4e2f-ba9d-61204c3f1aa0",
   "metadata": {},
   "source": [
    "# Super Mario Bros Reflexive Agent\n",
    "<ol type=\"I\">\n",
    "<li><a href='#short-overview'>Short Overview</a></li>\n",
    "<li><a href='#approach'>Approach</a>\n",
    "  <ol type=\"A\">\n",
    "    <li><a href='#random-agent'>Random Agent</a></li>\n",
    "    <li><a href='#heuristic-agent'>Heuristic Agent</a></li>\n",
    "    <li><a href='#basic-ppo-agent'>Basic PPO Agent</a></li>\n",
    "    <li><a href='#q-learning-agent'>Q-Learning Agent</a></li>\n",
    "  </ol>\n",
    "</li>\n",
    "<li><a href='#results'>Results</a></li>\n",
    "<li><a href='#conclusion'>Conclusion</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99cf0f-1b67-44ab-a413-64bd96974842",
   "metadata": {},
   "source": [
    "## Short Overview\n",
    "\n",
    "*short overview of what your project is about (e.g. you're building /testing certain RL models in certain environments; yes you can test your algorithm in more than 1 environment if your goal is to test an algorithm(s) performances in different settings)*\n",
    "\n",
    "1. Does it include the clear overview on what the project is about? (4)\n",
    "\n",
    "2. Does it explain how the environment works and what the game rules are? (4)\n",
    "\n",
    "For my project, I chose to teach a learning model to play the original Super Mario Bros. game for the NES. I utilized a library created by Christian Kauten called gym-super-mario-bros, which provides an OpenAI Gym environment using the nes-py emulator (Kauten, 2018). The challenge is to beat as many levels as possible in the original Mario game for NES with the following rules of the game.\n",
    "\n",
    "The goal of the game is to avoid enemies and pits to reach the end of each level. One hit and Mario loses a life, starting over from the nearest checkpoint. Power-ups provide Mario an additional hit. The following page from the original game manual outlines the inputs Mario receives for the game:\n",
    "\n",
    "![image](images/controls.jpg)\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "The game environment takes these controls and creates the following action lists that can be used within the environment wrapper:\n",
    "\n",
    "```python\n",
    "# actions for the simple run right environment\n",
    "RIGHT_ONLY = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for very simple movement\n",
    "SIMPLE_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for more complex movement\n",
    "COMPLEX_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "    ['left', 'A'],\n",
    "    ['left', 'B'],\n",
    "    ['left', 'A', 'B'],\n",
    "    ['down'],\n",
    "    ['up'],\n",
    "]\n",
    "```\n",
    "\n",
    "The environment can also determine the following keys for the gamestate:\n",
    "\n",
    "| Key       | Type | Description                                |\n",
    "|-----------|------|--------------------------------------------|\n",
    "| coins     | int  | The number of collected coins              |\n",
    "| flag_get  | bool | True if Mario reached a flag or ax         |\n",
    "| life      | int  | The number of lives left, i.e., {3, 2, 1}  |\n",
    "| score     | int  | The cumulative in-game score               |\n",
    "| stage     | int  | The current stage, i.e., {1, ..., 4}       |\n",
    "| status    | str  | Mario's status, i.e., {'small', 'tall', 'fireball'} |\n",
    "| time      | int  | The time left on the clock                 |\n",
    "| world     | int  | The current world, i.e., {1, ..., 8}       |\n",
    "| x_pos     | int  | Mario's x position in the stage (from the left) |\n",
    "| y_pos     | int  | Mario's y position in the stage (from the bottom) |\n",
    "\n",
    "Additionally, the environment utilizes the following parameters for the reward function:\n",
    "\n",
    "v: the difference in agent x values between states\n",
    "\n",
    "c: the difference in the game clock between frames\n",
    "\n",
    "d: a death penalty that penalizes the agent for dying in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aeec23-5c3b-4af6-a8c8-ee157900fc45",
   "metadata": {},
   "source": [
    "<a id='approach'></a>\n",
    "\n",
    "## Approach\n",
    "\n",
    "*explain your environment, your choice of model(s), the methods and purpose of testing and experiments, explain any trouble shooting required.*\n",
    "\n",
    "3. Does it explain clearly the model(s) of choices, the methods and purpose of tests and experiments? (7)\n",
    "\n",
    "4. Does it show problem solving procedure- e.g. how the author solved and improved when an algorithm doesn't work well. Note that it's not about debugging or programming/implementation, but about when a correctly implemented algorithm wasn't enough for the problem and the author had to modify/add some features or techniques, or compare with another model, etc. (7)\n",
    "\n",
    "The initial setup for the environment was a bit tricky due to some incompatibilities between the chosen gym library gym-super-mario-bros JoypadSpace wrapper and the current version of OpenAi's gym framework, specifically with the `reset` method. Huge thanks to NathanGavinski who supplied [a workaround](https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091) in the issues forum for gym-super-mario-bros Git. (NathanGavenski, 2023).\n",
    "\n",
    "The following code utilizes this fix along with the suggested boilerplate setup from the gym-super-mario-bros documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "05dc07bf-8674-4287-878a-569fd9ab9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:568: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:619: UserWarning: \u001b[33mWARN: Env check failed with the following message: Calling the reset method with `return_info=True` did not return a 2-tuple\n",
      "You can set `disable_env_checker=True` to disable this check.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gymnasium.wrappers import StepAPICompatibility, TimeLimit\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2b346",
   "metadata": {},
   "source": [
    "## Random Agent\n",
    "\n",
    "Let's create an agent that makes random movements just to make sure our environment is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "de8bc496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 596\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "done = True\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "# Run the environment for 5000 steps\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    done = done or truncated\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.05 seconds between frames\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27aa4a",
   "metadata": {},
   "source": [
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"./images/random_agent.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "You can see that the random agent never gets past the second pipe. This is because it is not probabilistically reasonable for random inputs to know to sustain a jump by pressing A to get high enough to clear the pipe and keep going. This pipe exists at an X value of 595. Let's see if there are any other agents that can get farther."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a4f41",
   "metadata": {},
   "source": [
    "## Heuristic Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489cad9-4b55-4b39-bf99-e12bab7547da",
   "metadata": {},
   "source": [
    "To get a baseling, I decided to implement a basic heuristic model that uses a simple algorithm to try to beat a level of Super Mario Bros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7347e250-b94d-4483-8810-fdcfe114b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 320\n",
      "Max World: 1\n",
      "Max Stage: 2\n"
     ]
    }
   ],
   "source": [
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)\n",
    "\n",
    "# create global variables for inputs\n",
    "done = True\n",
    "going_up = False\n",
    "prev_y = None\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "for step in range(1700):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        prev_y = None\n",
    "        hold_jump = False\n",
    "    \n",
    "    # if Mario is on flat groun\n",
    "    # or in the process of rising from previous jump\n",
    "    # will continue to hold A to perform the maximum jump\n",
    "    action = SIMPLE_MOVEMENT.index(['right', 'A', 'B']) if going_up else SIMPLE_MOVEMENT.index(['right', 'B'])\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # set going_up to true if Mario is not descending\n",
    "    if prev_y is not None:\n",
    "        if info['y_pos'] >= prev_y:\n",
    "            going_up = True\n",
    "        else:\n",
    "            going_up = False\n",
    "\n",
    "    # capture current y position to compare for next state\n",
    "    prev_y = info['y_pos']\n",
    "        \n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "\n",
    "    if done or truncated:\n",
    "        done = True\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.01 seconds between frames\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4640cca",
   "metadata": {},
   "source": [
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"./images/heuristic_agent.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "This heuristic, while not an actual learning model, is effective by sheer luck. This strategy is similar to what an actual player might try when attempting to beat the first level. It is intuitive to try and jump as high and as often as possible to clear most obstacles and enemies. It is by sheer luck (and some deaths) that the heuristic is able to avoid enemies and pits. Despite this, however, this agent is able to clear the first level, although it does not get very far in the second level.\n",
    "\n",
    "Let's try to create a learning model and see if the agent can beat the level without relying on luck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4371",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5b078",
   "metadata": {},
   "source": [
    "\n",
    "Let's actually have an agent that learns, rather than blindly jumping. We'll start by creating a basic Proximal Policy Optimization (PPO) that makes stable updates to policies by limiting drastic changes, ensuring efficient learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9957fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  raise ModuleNotFoundError(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 103 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 1   |\n",
      "|    total_timesteps | 128 |\n",
      "----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 256         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033081587 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | -0.00124    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 192         |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 425         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 20        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 18        |\n",
      "|    total_timesteps      | 384       |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0200424 |\n",
      "|    clip_fraction        | 0.0996    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.9      |\n",
      "|    explained_variance   | 0.0105    |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 20.4      |\n",
      "|    n_updates            | 8         |\n",
      "|    policy_gradient_loss | -0.00349  |\n",
      "|    value_loss           | 168       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 512        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06563064 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.85      |\n",
      "|    explained_variance   | 0.069      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 45.8       |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | 0.0115     |\n",
      "|    value_loss           | 98.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 640         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019646216 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 34.6        |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 0.000365    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 47         |\n",
      "|    total_timesteps      | 768        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16857654 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.53      |\n",
      "|    explained_variance   | -0.0736    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 172        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.0216     |\n",
      "|    value_loss           | 247        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 896          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056644334 |\n",
      "|    clip_fraction        | 0.0801       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.00282     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 36.8         |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    value_loss           | 145          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015888235 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.0211     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 58.4        |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.000523   |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 1152         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054389173 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -0.00162     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 5.19         |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    value_loss           | 44.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 1280         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021641203 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.0177      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | 0.00707      |\n",
      "|    value_loss           | 70.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 1408        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015707789 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 0.0157      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.93        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 23          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 103        |\n",
      "|    total_timesteps      | 1536       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00930596 |\n",
      "|    clip_fraction        | 0.0215     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | -0.0179    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 7.64       |\n",
      "|    n_updates            | 44         |\n",
      "|    policy_gradient_loss | -0.00553   |\n",
      "|    value_loss           | 37.8       |\n",
      "----------------------------------------\n",
      "Mean reward: 680.0 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "env = StepAPICompatibility(env, output_truncation_bool=True)\n",
    "\n",
    "model = PPO(\n",
    "    'CnnPolicy',      # Use a convolutional neural network\n",
    "    env,              # environment\n",
    "    verbose=1,        # print diagnostics\n",
    "    learning_rate=1e-4,  # controls how much to adjust the model with each step\n",
    "    n_steps=128,      # affects the frequency of updates\n",
    "    batch_size=64,    # number of samples per gradient update\n",
    "    n_epochs=4,       # Number of epochs\n",
    "    clip_range=0.2,   # helps in limiting updates for stable training\n",
    "    ent_coef=0.03     # use entropy to encourage exploration\n",
    ")\n",
    "\n",
    "# Define evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=500, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./models/', name_prefix='ppo_mario')\n",
    "\n",
    "model.learn(total_timesteps=1500)\n",
    "\n",
    "# model = PPO.load(\"ppo_mario.zip\")\n",
    "\n",
    "model.save(\"./trained_agents/ppo_mario\")\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6daca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 595\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)\n",
    "\n",
    "# model = PPO.load(\"ppo_mario.zip\")\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "\n",
    "obs, info = env.reset()\n",
    "for step in range(1500):\n",
    "    action, _states = model.predict(obs.copy())\n",
    "    action = action.item()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4def3",
   "metadata": {},
   "source": [
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"./images/PPO_agent.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "This PPO agent is, unfortunately, unable to clear the pipe at the 595 X value. This fairs about as well as the random agent, and not as good as the heuristic agent. Given more timesteps, the agent could learn how to get farther, however let us try to create better reflexive learning agent using the Q learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3c39e",
   "metadata": {},
   "source": [
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c92186",
   "metadata": {},
   "source": [
    "Now, let us create our own Q-Learning agent. This is the meat of this project, so get ready to strap in. A huge thank you to Sourish Kundu, who made a video on how to do exactly this with some great explanations (Kundo, 2023). If you have a chance, I highly recommend giving it a watch, even if you are not working with this gym environment, as it gives some great explanations of general concepts we have been discussing in class.\n",
    "\n",
    "https://www.youtube.com/watch?v=_gmQZToTMac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe369c",
   "metadata": {},
   "source": [
    "We can begin by creating our own environment wrapper to skip four frames. Additionally, we will reduce the frame resolution, convert the frames to grayscale, and stack four frames. This will assist in reducing the complexity of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "69e710d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Wrapper\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        # create a reward accumulator\n",
    "        reward_accum = 0.0\n",
    "        done = False\n",
    "        for _ in range(self.skip):\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            # add reward to accumulator\n",
    "            reward_accum += reward\n",
    "            if done:\n",
    "                break\n",
    "        return next_state, reward_accum, done, info\n",
    "    \n",
    "    '''\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        print(state)\n",
    "        # Assuming we want to stack 'skip' frames\n",
    "        state = [state] * self.skip\n",
    "        state = np.stack(state, axis=0)\n",
    "        return state\n",
    "    '''\n",
    "\n",
    "def apply_wrappers(env):\n",
    "    env = SkipFrame(env, skip=4) # skip every four frames\n",
    "    env = ResizeObservation(env, shape=84) # reduce size of frame image\n",
    "    env = GrayScaleObservation(env) # create grayscale images\n",
    "    env = FrameStack(env, num_stack=4, lz4_compress=True) # stack frames (4 skipped)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617c80e",
   "metadata": {},
   "source": [
    "Next, we will create our own neural network agent using convolution layers and linear layers. We are using Conv2d layers with Relu activation functions, as these are typically appropriate for images (as I learned from HW5). Using MaxPool2d will reduce spatial dimensions and lower computational cost, which will hopefully lower the processing time for our algorithm.\n",
    "\n",
    "Additional methods are used to dynamically evaluate the input shape for our initial linear layer (`_get_conv_out`), prevent pytorch from updating the gradients if frozen (`_freeze`), and tell pytorch how to handle the forward pass for each tensor (`forward`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48be6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class AgentNN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, freeze=False):\n",
    "        super().__init__()\n",
    "        # Conolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # use built-in method to get the dimensional input size for initial linear layer\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        # Fully connected linear layers\n",
    "        self.network = nn.Sequential(\n",
    "            self.conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions) # determine best action to predict\n",
    "        )\n",
    "\n",
    "        # call the freeze method if frozen\n",
    "        # to make sure no parameters are updated if frozen\n",
    "        if freeze:\n",
    "            self._freeze()\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # try to use the GPU if possible\n",
    "        self.to(self.device)\n",
    "\n",
    "    # method to handle forward pass \n",
    "    def forward(self, x):\n",
    "        # pass the input tensor through the neural network layers\n",
    "        return self.network(x)\n",
    "\n",
    "    # get the number of neurons for our linear layers\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv_layers(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    # method to make sure gradients are not calculated if frozen\n",
    "    def _freeze(self):        \n",
    "        for p in self.network.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f3f35",
   "metadata": {},
   "source": [
    "Next, we will create our agent class to use the neural network created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32430615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tensordict import TensorDict # use tensors in python lists to speed up processing\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 input_dims, \n",
    "                 num_actions, \n",
    "                 lr=0.00025, \n",
    "                 gamma=0.9, \n",
    "                 epsilon=1.0, \n",
    "                 eps_decay=0.99999975, \n",
    "                 eps_min=0.1, \n",
    "                 replay_buffer_capacity=75000, \n",
    "                 batch_size=32, \n",
    "                 sync_network_rate=10000\n",
    "                 ):\n",
    "        \n",
    "        self.num_actions = num_actions # use the appropriate number of actions (SIMPLE_MOVEMENT dict has 7)\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_network_rate = sync_network_rate\n",
    "\n",
    "        # Networks\n",
    "        self.online_network = AgentNN(input_dims, num_actions)\n",
    "        self.target_network = AgentNN(input_dims, num_actions, freeze=True)\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.MSELoss() # loss function\n",
    "\n",
    "        # Replay buffer\n",
    "        storage = LazyMemmapStorage(replay_buffer_capacity)\n",
    "        self.replay_buffer = TensorDictReplayBuffer(storage=storage)\n",
    "        self.log_memory_usage()\n",
    "\n",
    "    def log_memory_usage(self):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_info = process.memory_info()\n",
    "        print(f\"Memory Usage: {mem_info.rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # create the potential to choose a random action\n",
    "        # this will include some value of randomness to increase exploration\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        \n",
    "        observation = (\n",
    "            torch.tensor(np.array(observation), dtype=torch.float32) # speed up processing by using tensors instead of numpy arrays\n",
    "            .unsqueeze(0) # add dimension of batch size to first index of tensor\n",
    "            .to(self.online_network.device) # move to the correct device (GPU or CPU)\n",
    "        )\n",
    "        # return the action with the highest Q-value\n",
    "        return self.online_network(observation).argmax().item()\n",
    "    \n",
    "    # compute the value of epsilon to diminish rewards for later actions\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min)\n",
    "\n",
    "    # put tensors in a dict and add to buffer\n",
    "    def store_in_memory(self, state, action, reward, next_state, done):\n",
    "        # Create TensorDict with correct shapes and types\n",
    "        data = TensorDict({\n",
    "            \"state\": torch.tensor(np.array(state), dtype=torch.float32),\n",
    "            \"action\": torch.tensor(action),\n",
    "            \"reward\": torch.tensor(reward),\n",
    "            \"next_state\": torch.tensor(np.array(next_state), dtype=torch.float32),\n",
    "            \"done\": torch.tensor(done)\n",
    "        }, batch_size=[])\n",
    "        self.replay_buffer.add(data)\n",
    "    \n",
    "    # copy weights of online network to target network if enough steps have passed\n",
    "    def sync_networks(self):\n",
    "        if self.learn_step_counter % self.sync_network_rate == 0 and self.learn_step_counter > 0:\n",
    "            self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "\n",
    "    # save current model (in case something goes wrong)\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.online_network.state_dict(), path)\n",
    "\n",
    "    # load model\n",
    "    def load_model(self, path):\n",
    "        self.online_network.load_state_dict(torch.load(path))\n",
    "        self.target_network.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        # if not enough experiences, return and keep going\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # copy weights to target network\n",
    "        self.sync_networks()\n",
    "        \n",
    "        # clear gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sample the replay buffer and store the results\n",
    "        samples = self.replay_buffer.sample(self.batch_size).to(self.online_network.device)\n",
    "        states = samples['state']\n",
    "        actions = samples['action']\n",
    "        rewards = samples['reward']\n",
    "        next_states = samples['next_state']\n",
    "        dones = samples['done']\n",
    "\n",
    "        # get the predicted values from our neural network with the appropriate batch size\n",
    "        predicted_q_values = self.online_network(states)\n",
    "        predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
    "\n",
    "        # Max returns two tensors, the first one is the maximum value, the second one is the index of the maximum value\n",
    "        target_q_values = self.target_network(next_states).max(dim=1)[0]\n",
    "        # The rewards of any future states don't matter if the current state is a terminal state\n",
    "        # If done is true, then 1 - done is 0, so the part after the plus sign (representing the future rewards) is 0\n",
    "        target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
    "\n",
    "        loss = self.loss(predicted_q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "        self.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058d5e2",
   "metadata": {},
   "source": [
    "We now have everything we need to create the environment and run our reinforcement learning architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d7554543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 4956.05 MB\n",
      "Currently processing episode 5/5 [########################################] 100.00%"
     ]
    }
   ],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
    "SHOULD_TRAIN = True\n",
    "DISPLAY = True\n",
    "NUM_OF_EPISODES = 5\n",
    "\n",
    "env = gym_super_mario_bros.make(ENV_NAME)\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "\n",
    "def print_progress(cur,end, bar_length=40):\n",
    "    progress = cur / end\n",
    "    block = int(round(bar_length * progress))\n",
    "    text = f\"\\rCurrently processing episode {cur}/{end} [{'#' * block + '-' * (bar_length - block)}] {progress * 100:.2f}%\"\n",
    "    print(text, end='', flush=True)\n",
    "\n",
    "for i in range(NUM_OF_EPISODES):\n",
    "    print_progress(i+1,NUM_OF_EPISODES)\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        a = agent.choose_action(state)\n",
    "        new_state, reward, done, info  = env.step(a)\n",
    "        \n",
    "        agent.store_in_memory(state, a, reward, new_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb3a1bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 5874.73 MB\n",
      "Max X: 898\n",
      "Max World: 1\n",
      "Max Stage: 2\n"
     ]
    }
   ],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
    "SHOULD_TRAIN = True\n",
    "DISPLAY = True\n",
    "NUM_OF_EPISODES = 5\n",
    "\n",
    "env = gym_super_mario_bros.make(ENV_NAME)\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "\n",
    "state = env.reset()\n",
    "for step in range(1500):\n",
    "    action = agent.choose_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28095c2-22be-4762-812f-d5210cd89f2b",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28409ff",
   "metadata": {},
   "source": [
    "\n",
    "*show the result and interpretation of your experiment. Any iterative improvements summary.*\n",
    "\n",
    "5. Does it include the results summary, interpretation of experiments and visualization (e.g. performance comparison table, graphs etc)? (7)\n",
    "\n",
    "\n",
    "| Name of Agent    | Max Level | Max World | Max X Position |\n",
    "|------------------|----------------|-----------|-----------|\n",
    "| Random Agent     | 1            | 1         | 595         |\n",
    "| Heuristic Agent  | 1            | 2         | 320         |\n",
    "| Basic PPO Agent  | 1            | 1         | 595         |\n",
    "| Q Learning Agent | ???            | ???         | ???         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455bdd-f926-4764-83ef-dbd6fc961590",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e061287",
   "metadata": {},
   "source": [
    "\n",
    "*Conclusion, discussion, reflection, or suggestions for future improvements or future ideas.*\n",
    "\n",
    "6. Does it include discussion (what went well or not and why), and suggestions for improvements or future work? (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6817d-c989-41d6-9a06-2c128b50ae0a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*Reference: Please include all relevant links (git, video, etc)*\n",
    "\n",
    "7. Does it include all deliverables (3)\n",
    "\t- git with codes or notebooks\n",
    "\t- writeup (you can consider notebook as a writeup if the notebook contains all needed contents and explanation)\n",
    "\t- demo clips\n",
    "\t- proper quote or reference\n",
    "    \n",
    "Kauten, C. (2018). Super Mario Bros for OpenAI Gym. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "NathanGavenski. (2023). Comment on issue #128 in Kautenja/gym-super-mario-bros repository. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091\n",
    "\n",
    "Kundu, S. (2023, October 2). Train AI to beat Super Mario Bros! || Reinforcement learning completely from scratch [Video]. YouTube. https://www.youtube.com/watch?v=_gmQZToTMac\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b94f9b-15f1-48eb-8b0d-8584b7a585a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

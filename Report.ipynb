{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffc2037-5366-430d-bb52-2170e163d598",
   "metadata": {},
   "source": [
    "GitHub Repository for this project can be found at https://github.com/seel6470/CSPB-3202-Final-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8866730-fe6d-4e2f-ba9d-61204c3f1aa0",
   "metadata": {},
   "source": [
    "# Beating Super Mario Bros 1-1 With Reinforcement Learning\n",
    "<ol type=\"I\">\n",
    "<li><a href='#short-overview'>Short Overview</a></li>\n",
    "<li><a href='#approach'>Approach</a>\n",
    "  <ol type=\"A\">\n",
    "    <li><a href='#random-agent'>Random Agent</a></li>\n",
    "    <li><a href='#heuristic-agent'>Heuristic Agent</a></li>\n",
    "    <li><a href='#ppo-agent'>PPO Agent</a></li>\n",
    "    <li><a href='#q-learning-agent'>Q-Learning Agent</a></li>\n",
    "  </ol>\n",
    "</li>\n",
    "<li><a href='#results'>Results</a></li>\n",
    "<li><a href='#conclusion'>Conclusion</a></li>\n",
    "<li><a href='#references'>References</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99cf0f-1b67-44ab-a413-64bd96974842",
   "metadata": {},
   "source": [
    "<a id='short-overview'></a>\n",
    "## Short Overview\n",
    "\n",
    "*short overview of what your project is about (e.g. you're building /testing certain RL models in certain environments; yes you can test your algorithm in more than 1 environment if your goal is to test an algorithm(s) performances in different settings)*\n",
    "\n",
    "1. Does it include the clear overview on what the project is about? (4)\n",
    "\n",
    "2. Does it explain how the environment works and what the game rules are? (4)\n",
    "\n",
    "For my project, I chose to teach a learning model to play the original Super Mario Bros. game for the NES. I utilized a library created by Christian Kauten called gym-super-mario-bros, which provides an OpenAI Gym environment using the nes-py emulator (Kauten, 2018). The challenge is to beat as many levels as possible in the original Mario game for NES with the following rules of the game.\n",
    "\n",
    "The goal of the game is to avoid enemies and pits to reach the end of each level. One hit and Mario loses a life, starting over from the nearest checkpoint. Power-ups provide Mario an additional hit. The following page from the original game manual outlines the inputs Mario receives for the game:\n",
    "\n",
    "![image](images/controls.jpg)\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "The game environment takes these controls and creates the following action lists that can be used within the environment wrapper:\n",
    "\n",
    "```python\n",
    "# actions for the simple run right environment\n",
    "RIGHT_ONLY = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for very simple movement\n",
    "SIMPLE_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for more complex movement\n",
    "COMPLEX_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "    ['left', 'A'],\n",
    "    ['left', 'B'],\n",
    "    ['left', 'A', 'B'],\n",
    "    ['down'],\n",
    "    ['up'],\n",
    "]\n",
    "```\n",
    "\n",
    "The environment can also determine the following keys for the gamestate:\n",
    "\n",
    "| Key       | Type | Description                                |\n",
    "|-----------|------|--------------------------------------------|\n",
    "| coins     | int  | The number of collected coins              |\n",
    "| flag_get  | bool | True if Mario reached a flag or ax         |\n",
    "| life      | int  | The number of lives left, i.e., {3, 2, 1}  |\n",
    "| score     | int  | The cumulative in-game score               |\n",
    "| stage     | int  | The current stage, i.e., {1, ..., 4}       |\n",
    "| status    | str  | Mario's status, i.e., {'small', 'tall', 'fireball'} |\n",
    "| time      | int  | The time left on the clock                 |\n",
    "| world     | int  | The current world, i.e., {1, ..., 8}       |\n",
    "| x_pos     | int  | Mario's x position in the stage (from the left) |\n",
    "| y_pos     | int  | Mario's y position in the stage (from the bottom) |\n",
    "\n",
    "Additionally, the environment utilizes the following parameters for the reward function:\n",
    "\n",
    "1. **v**: the difference in agent x values between states\n",
    "   - In this case, this is instantaneous velocity for the given step\n",
    "   - \\( v = x1 - x0 \\)\n",
    "     - \\( x0 \\) is the x position before the step\n",
    "     - \\( x1 \\) is the x position after the step\n",
    "   - Moving right \\( \\implies v > 0 \\)\n",
    "   - Moving left \\( \\implies v < 0 \\)\n",
    "   - Not moving \\( \\implies v = 0 \\)\n",
    "\n",
    "2. **c**: the difference in the game clock between frames\n",
    "   - The penalty prevents the agent from standing still\n",
    "   - \\( c = c0 - c1 \\)\n",
    "     - \\( c0 \\) is the clock reading before the step\n",
    "     - \\( c1 \\) is the clock reading after the step\n",
    "   - No clock tick \\( \\implies c = 0 \\)\n",
    "   - Clock tick \\( \\implies c < 0 \\)\n",
    "\n",
    "3. **d**: a death penalty that penalizes the agent for dying in a state\n",
    "   - This penalty encourages the agent to avoid death\n",
    "   - Alive \\( \\implies d = 0 \\)\n",
    "   - Dead \\( \\implies d = -15 \\)\n",
    "\n",
    "\\( r = v + c + d \\)\n",
    "\n",
    "The reward is clipped into the range \\([-15, 15]\\).\n",
    "\n",
    "In addition to the gym-super-mario-bros library, nes-py is required in order to emulate the NES in Python. Huge thanks to Kautenja for creating the environment and making this project possible! (Kauten, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aeec23-5c3b-4af6-a8c8-ee157900fc45",
   "metadata": {},
   "source": [
    "<a id='approach'></a>\n",
    "\n",
    "## Approach\n",
    "\n",
    "*explain your environment, your choice of model(s), the methods and purpose of testing and experiments, explain any trouble shooting required.*\n",
    "\n",
    "3. Does it explain clearly the model(s) of choices, the methods and purpose of tests and experiments? (7)\n",
    "\n",
    "4. Does it show problem solving procedure- e.g. how the author solved and improved when an algorithm doesn't work well. Note that it's not about debugging or programming/implementation, but about when a correctly implemented algorithm wasn't enough for the problem and the author had to modify/add some features or techniques, or compare with another model, etc. (7)\n",
    "\n",
    "The initial setup for the environment was a bit tricky due to some incompatibilities between the chosen gym library gym-super-mario-bros JoypadSpace wrapper and the current version of OpenAi's gym framework, specifically with the `reset` method. Huge thanks to NathanGavinski who supplied [a workaround](https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091) in the issues forum for gym-super-mario-bros Git. (NathanGavenski, 2023).\n",
    "\n",
    "The following code utilizes this fix along with the suggested boilerplate setup from the gym-super-mario-bros documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05dc07bf-8674-4287-878a-569fd9ab9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gymnasium.wrappers import StepAPICompatibility, TimeLimit\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2b346",
   "metadata": {},
   "source": [
    "<a id='random-agent'></a>\n",
    "\n",
    "## Random Agent\n",
    "\n",
    "Let's create an agent that makes random movements just to make sure our environment is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de8bc496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 594\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "done = True\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "# Run the environment for 5000 steps\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    done = done or truncated\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.05 seconds between frames\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27aa4a",
   "metadata": {},
   "source": [
    "![image](images/random_agent.gif)\n",
    "\n",
    "You can see that the random agent never gets past the second pipe. This is because it is not probabilistically reasonable for random inputs to know to sustain a jump by pressing and holding A to get high enough to clear the pipe and keep going. This pipe exists at an X value of 595/596. Let's see if there are any other agents that can get farther."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a4f41",
   "metadata": {},
   "source": [
    "<a id='heuristic-agent'></a>\n",
    "\n",
    "## Heuristic Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489cad9-4b55-4b39-bf99-e12bab7547da",
   "metadata": {},
   "source": [
    "Although it is not reinforcement learning, I decided to implement a basic heuristic model that uses a simple algorithm to try to beat a level of Super Mario Bros which will serve as our milestone to beat. If our RL agent is able to surpass the heuristic agent, we will consider the RL agent to be a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7347e250-b94d-4483-8810-fdcfe114b78b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# if Mario is on flat groun\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# or in the process of rising from previous jump\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# will continue to hold A to perform the maximum jump\u001b[39;00m\n\u001b[0;32m     37\u001b[0m action \u001b[38;5;241m=\u001b[39m SIMPLE_MOVEMENT\u001b[38;5;241m.\u001b[39mindex([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m going_up \u001b[38;5;28;01melse\u001b[39;00m SIMPLE_MOVEMENT\u001b[38;5;241m.\u001b[39mindex([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 38\u001b[0m state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# set going_up to true if Mario is not descending\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\step_api_compatibility.py:53\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, returning 5 or 4 items depending on `output_truncation_bool`.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     step_returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_api_compatibility(\n\u001b[0;32m     55\u001b[0m         step_returns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_truncation_bool, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_vector_env\n\u001b[0;32m     56\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nes_py\\nes_env.py:300\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[0;32m    302\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)\n",
    "\n",
    "# create global variables for inputs\n",
    "done = True\n",
    "going_up = False\n",
    "prev_y = None\n",
    "min_y = 0\n",
    "max_y = 0\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "for step in range(1700):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        prev_y = None\n",
    "        hold_jump = False\n",
    "    \n",
    "    # if Mario is on flat groun\n",
    "    # or in the process of rising from previous jump\n",
    "    # will continue to hold A to perform the maximum jump\n",
    "    action = SIMPLE_MOVEMENT.index(['right', 'A', 'B']) if going_up else SIMPLE_MOVEMENT.index(['right', 'B'])\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # set going_up to true if Mario is not descending\n",
    "    if prev_y is not None:\n",
    "        if info['y_pos'] >= prev_y:\n",
    "            going_up = True\n",
    "        else:\n",
    "            going_up = False\n",
    "\n",
    "    # capture current y position to compare for next state\n",
    "    prev_y = info['y_pos']\n",
    "        \n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "\n",
    "    if done or truncated:\n",
    "        done = True\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.01 seconds between frames to slow down render\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4640cca",
   "metadata": {},
   "source": [
    "![image](images/heuristic_agent.gif)\n",
    "\n",
    "This heuristic, while not an actual learning model, is effective by sheer luck. This strategy is similar to what an actual player might try when attempting to beat the first level. It is intuitive to try and jump as high and as often as possible to clear most obstacles and enemies. It is by sheer luck (and some deaths) that the heuristic is able to avoid enemies and pits. Despite this, however, this agent is able to clear the first level, although it does not get very far in the second level.\n",
    "\n",
    "Let's try to create a reinforcement learning model and see if the agent can beat the level without relying on luck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4371",
   "metadata": {},
   "source": [
    "<a id='ppo-agent'></a>\n",
    "\n",
    "\n",
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5b078",
   "metadata": {},
   "source": [
    "\n",
    "Let's actually have an agent that learns, rather than blindly jumping. The initial model of choice will be a Proximal Policy Optimization (PPO) agent. This type of reinforcement learning model learns a policy that directly maps an approximation of the optimal action for any given state to exploit  to approximate the optimal action for any given state to maximize the reward returned over time.\n",
    "\n",
    "It does this by using gradient ascent to maximize the policy using the clipped objective function, which helps avoid making drastic weight updates:\n",
    "\n",
    "$L(\\theta) = \\mathbb{E} \\left[ \\min \\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}, \\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A} \\right) \\right]$\n",
    "\n",
    "This function seeks to optimize the parameters ($\\theta$) of the neural network by evaluating the expected value ($\\mathbb{E}$) of the minimum between the following two values:\n",
    "\n",
    "$$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}$$\n",
    "\n",
    "<p style=\"text-align: center;\">or</p>\n",
    "\n",
    "$$\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}$$\n",
    "\n",
    "$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}$ is the policy ratio, or how much the new policy $\\pi_\\theta$ has changed in relation to the old policy $\\pi_{old}$ and $\\hat A$ is the advantage, or how much better or worse the action $a$ is compared to the average action taken in state $s$\n",
    "\n",
    "The first term represents the weight change when the policy ratio is directly multiplied by the advantage. The second uses the clipping function to constrain the value of the policy ratio within the range $1 - \\epsilon$ and $1 + \\epsilon$ before multiplying with the advantage $\\hat A$\n",
    "\n",
    "Finding the expected value of the minimum of these two terms results in a gradient ascent that should prevent drastic policy updates and ensure more stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9957fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000002E952DAE5A0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002E952DAECF0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 128 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 3   |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 1024       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02697697 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.68      |\n",
      "|    explained_variance   | 0.00227    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.82       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.0105     |\n",
      "|    value_loss           | 132        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 1536        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010362429 |\n",
      "|    clip_fraction        | 0.0775      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.69       |\n",
      "|    explained_variance   | -0.543      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.561      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00155     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022481172 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | -0.0557      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.6         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 0.177        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055522034 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.69        |\n",
      "|    explained_variance   | -0.15        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.599       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088130066 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.691        |\n",
      "|    explained_variance   | -0.163        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.584        |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | 0.00113       |\n",
      "|    value_loss           | 0.114         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 68           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 3584         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018930949 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | -0.208       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.566       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 68           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029904246 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | 0.0025       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.614       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00481     |\n",
      "|    value_loss           | 0.114        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 68           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 4608         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034542074 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.57        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00095     |\n",
      "|    value_loss           | 0.094        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=741.00 +/- 0.00\n",
      "Episode length: 319.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 741         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017280929 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00737    |\n",
      "|    value_loss           | 48.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 60   |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 85   |\n",
      "|    total_timesteps | 5120 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 5632        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016778428 |\n",
      "|    clip_fraction        | 0.0883      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.64        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.0158      |\n",
      "|    value_loss           | 68.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042725027 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.0273      |\n",
      "|    value_loss           | 48.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 6656         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025254914 |\n",
      "|    clip_fraction        | 0.309        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | -0.809       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00688     |\n",
      "|    value_loss           | 4.43         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 116        |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00717536 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.68      |\n",
      "|    explained_variance   | -0.145     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.292      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | 0.00724    |\n",
      "|    value_loss           | 2.95       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 7680        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011942051 |\n",
      "|    clip_fraction        | 0.051       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | -0.363      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.472      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.00465     |\n",
      "|    value_loss           | 1.58        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 62           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003833631 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.692       |\n",
      "|    explained_variance   | -0.57        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.511       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | 0.000779     |\n",
      "|    value_loss           | 0.984        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 8704        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000938796 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | 0.0337      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.516      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -2.35e-05   |\n",
      "|    value_loss           | 0.953       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 60           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076097255 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | 0.537        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.98         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00472     |\n",
      "|    value_loss           | 28.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 60           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 161          |\n",
      "|    total_timesteps      | 9728         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031501849 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.69        |\n",
      "|    explained_variance   | 0.67         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.000648     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=737.00 +/- 0.00\n",
      "Episode length: 361.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 361          |\n",
      "|    mean_reward          | 737          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046295435 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.685       |\n",
      "|    explained_variance   | -0.435       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.69         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | 0.00457      |\n",
      "|    value_loss           | 4.19         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 56    |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 180   |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.06e+04    |\n",
      "|    ep_rew_mean          | 2.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 10752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036394875 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18          |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | 0.0306      |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.06e+04  |\n",
      "|    ep_rew_mean          | 2.49e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 56        |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 198       |\n",
      "|    total_timesteps      | 11264     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.2521572 |\n",
      "|    clip_fraction        | 0.912     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.254    |\n",
      "|    explained_variance   | 0.645     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 46.3      |\n",
      "|    n_updates            | 210       |\n",
      "|    policy_gradient_loss | 0.169     |\n",
      "|    value_loss           | 205       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.06e+04   |\n",
      "|    ep_rew_mean          | 2.49e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 207        |\n",
      "|    total_timesteps      | 11776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12667787 |\n",
      "|    clip_fraction        | 0.47       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.376     |\n",
      "|    explained_variance   | 0.127      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 36.9       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | 0.08       |\n",
      "|    value_loss           | 59.9       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.06e+04   |\n",
      "|    ep_rew_mean          | 2.49e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 215        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01825385 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.669     |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.5       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | 0.00992    |\n",
      "|    value_loss           | 30.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.06e+04    |\n",
      "|    ep_rew_mean          | 2.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 12800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017447434 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.44        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 26          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.06e+04     |\n",
      "|    ep_rew_mean          | 2.49e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 57           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 233          |\n",
      "|    total_timesteps      | 13312        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064301165 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | 0.672        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.294        |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 15.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.06e+04    |\n",
      "|    ep_rew_mean          | 2.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 13824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010564732 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.00494     |\n",
      "|    value_loss           | 3.38        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.06e+04     |\n",
      "|    ep_rew_mean          | 2.49e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 57           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 249          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022172811 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.685       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.38        |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 1.42         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.06e+04     |\n",
      "|    ep_rew_mean          | 2.49e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 57           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 257          |\n",
      "|    total_timesteps      | 14848        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076212995 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.69        |\n",
      "|    explained_variance   | 0.694        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.456       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 1.07         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=741.00 +/- 0.00\n",
      "Episode length: 319.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 319           |\n",
      "|    mean_reward          | 741           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 15000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034047035 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.691        |\n",
      "|    explained_variance   | 0.695         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.482        |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | 0.00044       |\n",
      "|    value_loss           | 0.871         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 124\u001b[0m\n\u001b[0;32m    121\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_agents\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m'\u001b[39m), log_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_agents\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m), eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    122\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m CheckpointCallback(save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25000\u001b[39m, save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_agents\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m'\u001b[39m), name_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_mario\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 124\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m700000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_agents\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_mario_2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Evaluate the agent\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:655\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[0;32m    654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[1;32m--> 655\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[0;32m    657\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:89\u001b[0m, in \u001b[0;36mDistribution.get_actions\u001b[1;34m(self, deterministic)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode()\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:298\u001b[0m, in \u001b[0;36mCategoricalDistribution.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample_shape, torch\u001b[38;5;241m.\u001b[39mSize):\n\u001b[0;32m    131\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m--> 132\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m    133\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs_2d, sample_shape\u001b[38;5;241m.\u001b[39mnumel(), \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\utils.py:148\u001b[0m, in \u001b[0;36mlazy_property.__get__\u001b[1;34m(self, instance, obj_type)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 148\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, value)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\categorical.py:101\u001b[0m, in \u001b[0;36mCategorical.probs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;129m@lazy_property\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprobs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlogits_to_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\utils.py:90\u001b[0m, in \u001b[0;36mlogits_to_probs\u001b[1;34m(logits, is_binary)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_binary:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(logits)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1888\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1886\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1888\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1890\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "\n",
    "import numpy as np\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "import gym # v0.26.2\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "from gym import Wrapper\n",
    "from gym import RewardWrapper\n",
    "import os\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        # create a reward accumulator\n",
    "        reward_accum = 0.0\n",
    "        done = False\n",
    "        for _ in range(self.skip):\n",
    "            next_state, reward, done, truncated, info = self.env.step(action)\n",
    "            # add reward to accumulator\n",
    "            reward_accum += reward\n",
    "            if done:\n",
    "                break\n",
    "        return next_state, reward_accum, done, truncated, info\n",
    "\n",
    "# final custom reward calculation after failed training (see below for details)\n",
    "class CustStepReward(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_y_pos = 0\n",
    "        self.reward_mean = 0\n",
    "        self.reward_var = 1\n",
    "        self.num_rewards = 0\n",
    "        self.succesive_A_presses = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        next_state, reward, done, truncated, info = self.env.step(action)\n",
    "        # if rising, agent must have pressed A state before\n",
    "        if self.prev_y_pos < info['y_pos']:\n",
    "            self.succesive_A_presses += 1\n",
    "            reward += 2 * self.succesive_A_presses # incentivize jumping higher if already jumping\n",
    "        # agent is not rising\n",
    "        else:\n",
    "            # reset to 0\n",
    "            # no reward (and no penalty)\n",
    "            self.succesive_A_presses = 0\n",
    "        self.prev_y_pos = info['y_pos']\n",
    "        \n",
    "        if info['life'] < 2:\n",
    "            reward -= 50 # heavily penalize death and end the episode\n",
    "            done = True\n",
    "        if info['flag_get']:\n",
    "            reward += 100 # heavily incentivize beating the level\n",
    "        \n",
    "        # Update reward statistics\n",
    "        self.num_rewards += 1\n",
    "        old_mean = self.reward_mean\n",
    "        self.reward_mean += (reward - self.reward_mean) / self.num_rewards\n",
    "        self.reward_var += (reward - old_mean) * (reward - self.reward_mean)\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward_std = np.sqrt(self.reward_var / self.num_rewards)\n",
    "        normalized_reward = (reward - self.reward_mean) / (reward_std + 1e-8)\n",
    "        return next_state, normalized_reward, done, truncated, info\n",
    "\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps\n",
    "\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, CUSTOM_ACTIONS)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "env = StepAPICompatibility(env, output_truncation_bool=True)\n",
    "#env = CustStepReward(env)\n",
    "\n",
    "#env = SkipFrame(env, skip=4)\n",
    "#env = ResizeObservation(env, shape=84) # reduce size of frame image\n",
    "#env = GrayScaleObservation(env) # create grayscale images\n",
    "#env = FrameStack(env, num_stack=8, lz4_compress=True) # stack frames\n",
    "\n",
    "model = PPO(\n",
    "    'CnnPolicy',      # Use a convolutional neural network\n",
    "    env,              # Game environment\n",
    "    verbose=1,        # print diagnostics\n",
    "    learning_rate=3e-4,  # how much to adjust the model with each step\n",
    "    n_steps=512,      # frequency of updates\n",
    "    batch_size=128,   # number of state samples evaluated in clipping objective function\n",
    "    clip_range = 0.2, # range to clip policy ratio in the clipping objective function\n",
    "    ent_coef = 0.9,   # entropy coefficient to encourage exploration\n",
    "    gamma = 0.99      # diminishes rewards for future action-state returns\n",
    ")\n",
    "\n",
    "\n",
    "# uncomment if continuing a previous training session\n",
    "#model.load(os.path.join(\"trained_agents\",\"ppo\",\"ppo_mario.zip\"))\n",
    "\n",
    "# Define evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=os.path.join('trained_agents','ppo'), log_path=os.path.join('trained_agents','ppo','logs'), eval_freq=5000, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=25000, save_path=os.path.join('trained_agents','ppo'), name_prefix='ppo_mario')\n",
    "\n",
    "model.learn(total_timesteps=700000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "model.save(os.path.join('trained_agents','ppo','ppo_mario_2'))\n",
    "\n",
    "# Evaluate the agent\n",
    "print(\"evaluating policy...\")\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "env.close()\n",
    "print(f\"Mean reward: {mean_reward} ± {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fdaf2",
   "metadata": {},
   "source": [
    "To optimize the learning process, I decided to use several wrappers for optimization. The idea for these optimizations came from a video by Sourish Kundu I used as the basis for the Q-Learning model implemented later in this project (Kundu, 2023). The following additions implemented above along with configuring Stable Baselines to use the GPU sped up the learning process quite a bit and allowed me to train the model for 400,000 episodes over the course of only a few hours.\n",
    "\n",
    "The first optimization is to create a custom wrapper that skips every four frames. Since the NES Mario Bros runs at about 60 fps, not much changes in the game state for each frame and the agent will choose an action and repeat the action for every four frames.\n",
    "\n",
    "Additional optimizations included downgrading from color RGB pixel values to gray scale values, reducing the image size, and stacking 8 frames at a time. The framestacking, when combined with the frame skipping, reduces the processing complexity of the model and at the same time allow it to interpret more meaningful game state changes than it would otherwise evaluate from every individual frame.\n",
    "\n",
    "The actual architecture of the model uses a convolutional neural network __use model.policy to determine model architecture__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6daca08",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m action, info \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(obs))\n\u001b[0;32m     47\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mitem()       \n\u001b[1;32m---> 48\u001b[0m obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m max_world:\n\u001b[0;32m     50\u001b[0m     max_world \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\frame_stack.py:173\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mSkipFrame.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip):\n\u001b[1;32m---> 31\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# add reward to accumulator\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     reward_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[28], line 51\u001b[0m, in \u001b[0;36mCustStepReward.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     50\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# if rising, agent must have pressed A state before\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_y_pos \u001b[38;5;241m<\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_pos\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:54\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, returning 5 or 4 items depending on `apply_step_compatibility`.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     step_returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_truncation_bool:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_terminated_truncated_step_api(step_returns)\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nes_py\\nes_env.py:300\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[0;32m    302\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "\n",
    "import numpy as np\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "env = StepAPICompatibility(env, output_truncation_bool=True)\n",
    "#env = CustStepReward(env)\n",
    "\n",
    "#env = SkipFrame(env, skip=4)\n",
    "#env = ResizeObservation(env, shape=84) # reduce size of frame image\n",
    "#env = GrayScaleObservation(env) # create grayscale images\n",
    "#env = FrameStack(env, num_stack=8, lz4_compress=True) # stack frames\n",
    "\n",
    "# uncomment to load saved trained model\n",
    "# model = PPO.load(os.path.join(\"trained_agents\",\"ppo\",\"ppo_mario_25000_steps.zip\"))\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "obs, info = env.reset()\n",
    "for step in range(2000):\n",
    "    action, info = model.predict(np.array(obs))\n",
    "    action = action.item()       \n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4def3",
   "metadata": {},
   "source": [
    "![image](images/initial_ppo.gif)\n",
    "\n",
    "With the default PPO model values, the agent is unable to clear the pipe at the 595 X coordinate. I allowed the agent to train for 100,000 episodes hoping it would figure out that keeping the A button held down would lead to a higher jump, allowing it to continue to the right. Unfortunately, after 100,000 iterations, the model instead runs directly into the first goomba over and over.\n",
    "\n",
    "![image](images/goomba_death.gif)\n",
    "\n",
    "I attempted to fine tune the learning rate using a scheduler, fine tuning the gamma and clip values, as well as included an entropy coefficient to encourage exploration to no avail. Even with a large entropy coefficient, the model was not able to get over the pipe by chance. In retrospect, this is consistent with the behavior with our random agent, as the chances of stringing together several A button presses while also holding right for forward momentum could be rather unlikely. Even with the addition of frame skipping, which causes the agent to perform an action for every four frames, the agent was unable to clear the pipe in any of the learning episodes.\n",
    "\n",
    "__Because of this, the agent always gets stuck at either the first or second pipe and accrues penalties for the clock ticking down with no forward progress, per the default reward structure. After 100,000 episodes, the agent seems to learn that in order to avoid this penalty, it is more optimal to run directly into the first goomba, receiving a smaller penalty from taking a death than the accrued penalty it receives from getting stuck at the pipe.__\n",
    "\n",
    "To counteract this, I added a new Gym environment wrapper that returns a new reward function. Learning from the heuristic agent, I believe Mario will both progress farther and avoid death more if he spends more time jumping. Because of this, I added a reward if Mario's change in Y-value is positive (meaning he is ascending). This should hopefully incentivize the agent to continuously press the A button and increase the chances that it is able to clear any tall pipes.\n",
    "\n",
    "Additionally, I chose to give a heavy penalty and terminate the episode when the agent dies, which should discourage the agent from exploiting any rewards it receives from taking an intentional death.\n",
    "\n",
    "Lastly, I added an incentive for the agent if the 'get_flag' bool is true, further incentivizing the agent to complete the level.\n",
    "\n",
    "Lastly, I normalized the returned reward based on the average reward of the episode in order to stabilize the learning process while I made adjustments:\n",
    "\n",
    "```python\n",
    "class CustStepReward(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_y_pos = 0\n",
    "        self.reward_mean = 0\n",
    "        self.reward_var = 1\n",
    "        self.num_rewards = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        next_state, reward, done, truncated, info = self.env.step(action)\n",
    "        # create custom reward value\n",
    "        if self.prev_y_pos < info['y_pos']:\n",
    "            reward += 4 # incentivize being airborn\n",
    "        self.prev_y_pos = info['y_pos']\n",
    "        if info['life'] < 2:\n",
    "            reward -= 50 # heavily penalize death and end the episode\n",
    "            done = True\n",
    "        if info['flag_get']:\n",
    "            reward += 100 # heavily incentivize beating the level\n",
    "        \n",
    "        # Update reward statistics\n",
    "        self.num_rewards += 1\n",
    "        old_mean = self.reward_mean\n",
    "        self.reward_mean += (reward - self.reward_mean) / self.num_rewards\n",
    "        self.reward_var += (reward - old_mean) * (reward - self.reward_mean)\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward_std = np.sqrt(self.reward_var / self.num_rewards)\n",
    "        normalized_reward = (reward - self.reward_mean) / (reward_std + 1e-8)\n",
    "        return next_state, normalized_reward, done, truncated, info\n",
    "```\n",
    "\n",
    "The end result worked as expected, and the agent was able to get farther in the level after training for 100,000 episodes. __However, after training for 400,000 episodes, the model again resorted to exploitation, this time using the reward for increasing y position. The reward it received for this outweighed the penalty for staying still and it found that the optimal policy was to stay in one place and jump continuously over and over to accrue rewards until the time ran out.__\n",
    "\n",
    "![image](images/left_model.gif)\n",
    "\n",
    "To fix this, I revised the reward function again to only reward increasing y position if already in a jumping state and scale this reward by the number of successive A presses. This should hopefully encourage the agent to jump higher to gain a higher reward:\n",
    "\n",
    "```python\n",
    "if self.prev_y_pos < info['y_pos']:\n",
    "    self.succesive_A_presses += 1\n",
    "    reward += 2 * self.succesive_A_presses # incentivize jumping higher if already jumping\n",
    "# agent is not rising\n",
    "else:\n",
    "    # reset to 0\n",
    "    # no reward (and no penalty)\n",
    "    self.succesive_A_presses = 0\n",
    "self.prev_y_pos = info['y_pos']\n",
    "```\n",
    "\n",
    "Additionally, I simplified the movement actions to only `Right + B` for running right, and `Right + B + A` for a running jump.\n",
    "\n",
    "```python\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "```\n",
    "\n",
    "While this simplification may make it harder for the agent to avoid enemies, it is more in alignment with the heuristic agent we used as a baseline and should hopefully allow the agent to make better progress.\n",
    "\n",
    "With these revisions, the agent was able to complete the level and even get a bit farther after 600,000 training steps.\n",
    "\n",
    "![image](images/ppo_final.gif)\n",
    "\n",
    "This reward function seems to work as intedned and the model learns with quite a bit of stability. Given enough iterations, I believe the model would continue to improve and progress farther into the game.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3c39e",
   "metadata": {},
   "source": [
    "<a id='q-learning-agent'></a>\n",
    "\n",
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c92186",
   "metadata": {},
   "source": [
    "To compare with the Stable Baseling3 PPO agent, I also create a Q-Learning agent. As referenced before, a huge thank you to Sourish Kundu, who made a video on how to do exactly this with some great explanations (Kundu, 2023). If you have a chance, I highly recommend giving it a watch, even if you are not working with this gym environment, as it gives some great explanations of general concepts we have been discussing in class.\n",
    "\n",
    "https://www.youtube.com/watch?v=_gmQZToTMac\n",
    "\n",
    "https://github.com/Sourish07/Super-Mario-Bros-RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe369c",
   "metadata": {},
   "source": [
    "We can begin by creating the same wrappers as before. Initially, I used these wrappers for the Q-Learning model and applied them to the PPO model after the fact. We again skip 4 frames, rescale and reduce the images to gray scale while stacking 4 frames. I am also including the previous reward wrapper used in the PPO model before applying all these wrappers together in the function `apply_wrappers` that returns the environment utilizing all of the environment wrappers.\n",
    "\n",
    "One note is that the following algorithm requires gym version 0.24.0 as the returns of the reset function need to be a single value, as opposed to the tuple returned in later versions. Overriding the reset function does not work, as several of the necessary wraooers and Python modules are not compatible with the newer versions of gym. If you are attempting to run this cell, you will receive an error unless you run `pip install gym==0.24.0` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gym import Wrapper\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        # create a reward accumulator\n",
    "        reward_accum = 0.0\n",
    "        done = False\n",
    "        for _ in range(self.skip):\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            # add reward to accumulator\n",
    "            reward_accum += reward\n",
    "            if done:\n",
    "                break\n",
    "        return next_state, reward_accum, done, info\n",
    "    \n",
    "class CustStepReward(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.prev_y_pos = 0\n",
    "        self.reward_mean = 0\n",
    "        self.reward_var = 1\n",
    "        self.num_rewards = 0\n",
    "        self.succesive_A_presses = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # if rising, agent must have pressed A state before\n",
    "        if self.prev_y_pos < info['y_pos']:\n",
    "            self.succesive_A_presses += 1\n",
    "            reward += 2 * self.succesive_A_presses # incentivize jumping higher if already jumping\n",
    "        # agent is not rising\n",
    "        else:\n",
    "            # reset to 0\n",
    "            # no reward (and no penalty)\n",
    "            self.succesive_A_presses = 0\n",
    "        self.prev_y_pos = info['y_pos']\n",
    "        \n",
    "        if info['life'] < 2:\n",
    "            reward -= 50 # heavily penalize death and end the episode\n",
    "            done = True\n",
    "        if info['flag_get']:\n",
    "            reward += 100 # heavily incentivize beating the level\n",
    "        \n",
    "        # Update reward statistics\n",
    "        self.num_rewards += 1\n",
    "        old_mean = self.reward_mean\n",
    "        self.reward_mean += (reward - self.reward_mean) / self.num_rewards\n",
    "        self.reward_var += (reward - old_mean) * (reward - self.reward_mean)\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward_std = np.sqrt(self.reward_var / self.num_rewards)\n",
    "        normalized_reward = float((reward - self.reward_mean) / (reward_std + 1e-8))\n",
    "        return next_state, normalized_reward, done, info\n",
    "\n",
    "def apply_wrappers(env):\n",
    "    env = SkipFrame(env, skip=4) # skip every four frames\n",
    "    env = ResizeObservation(env, shape=84) # reduce size of frame image\n",
    "    env = GrayScaleObservation(env) # create grayscale images\n",
    "    env = FrameStack(env, num_stack=4, lz4_compress=True) # stack frames (4 skipped)\n",
    "    env = CustStepReward(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617c80e",
   "metadata": {},
   "source": [
    "Instead of using a default neural network, we will create our own using convolution layers and linear layers. We are using Conv2d layers with Relu activation functions, as these are typically appropriate for images (as I learned from HW5). Using MaxPool2d will reduce spatial dimensions and lower computational cost, which will hopefully lower the processing time for our algorithm.\n",
    "\n",
    "Additional methods are used to dynamically evaluate the input shape for our initial linear layer (`_get_conv_out`), prevent pytorch from updating the gradients if frozen (`_freeze`), and tell pytorch how to handle the forward pass for each tensor (`forward`).\n",
    "\n",
    "I reduced the complexity of the architecture from Sourish's video to improve processing speed, but much of the structure of the attributes and methods were influenced heavily by his example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48be6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class AgentNN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, freeze=False):\n",
    "        super().__init__()\n",
    "        # Conolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # use built-in method to get the dimensional input size for initial linear layer\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        # Fully connected linear layers\n",
    "        self.network = nn.Sequential(\n",
    "            self.conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions) # determine best action to predict\n",
    "        )\n",
    "\n",
    "        # call the freeze method if frozen\n",
    "        # to make sure no parameters are updated if frozen\n",
    "        if freeze:\n",
    "            self._freeze()\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # try to use the GPU if possible\n",
    "        self.to(self.device)\n",
    "\n",
    "    # method to handle forward pass \n",
    "    def forward(self, x):\n",
    "        # pass the input tensor through the neural network layers\n",
    "        return self.network(x)\n",
    "\n",
    "    # get the number of neurons for our linear layers\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv_layers(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    # method to make sure gradients are not calculated if frozen\n",
    "    def _freeze(self):        \n",
    "        for p in self.network.parameters():\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f3f35",
   "metadata": {},
   "source": [
    "Next, we will create our agent class to use the neural network created previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32430615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tensordict import TensorDict # use tensors in python lists to speed up processing\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 input_dims, \n",
    "                 num_actions, \n",
    "                 lr=0.00025, \n",
    "                 gamma=0.9, \n",
    "                 epsilon=1.0, \n",
    "                 eps_decay=0.99999975, \n",
    "                 eps_min=0.1, \n",
    "                 replay_buffer_capacity=150000, \n",
    "                 batch_size=32, \n",
    "                 sync_network_rate=10000\n",
    "                 ):\n",
    "        \n",
    "        self.num_actions = num_actions # use the appropriate number of actions (SIMPLE_MOVEMENT dict has 7)\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_network_rate = sync_network_rate\n",
    "\n",
    "        # Networks\n",
    "        self.online_network = AgentNN(input_dims, num_actions)\n",
    "        self.target_network = AgentNN(input_dims, num_actions, freeze=True)\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.MSELoss() # loss function\n",
    "\n",
    "        # Replay buffer\n",
    "        storage = LazyMemmapStorage(replay_buffer_capacity)\n",
    "        self.replay_buffer = TensorDictReplayBuffer(storage=storage)\n",
    "        self.log_memory_usage()\n",
    "\n",
    "    def log_memory_usage(self):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_info = process.memory_info()\n",
    "        print(f\"Memory Usage: {mem_info.rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # create the potential to choose a random action\n",
    "        # this will include some value of randomness to increase exploration\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        \n",
    "        observation = (\n",
    "            torch.tensor(np.array(observation), dtype=torch.float32) # speed up processing by using tensors instead of numpy arrays\n",
    "            .unsqueeze(0) # add dimension of batch size to first index of tensor\n",
    "            .to(self.online_network.device) # move to the correct device (GPU or CPU)\n",
    "        )\n",
    "        # return the action with the highest Q-value\n",
    "        return self.online_network(observation).argmax().item()\n",
    "    \n",
    "    # compute the value of epsilon to diminish rewards for later actions\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min)\n",
    "\n",
    "    # put tensors in a dict and add to buffer\n",
    "    def store_in_memory(self, state, action, reward, next_state, done):\n",
    "        # Create TensorDict with correct shapes and types\n",
    "        data = TensorDict({\n",
    "            \"state\": torch.tensor(np.array(state), dtype=torch.float32),\n",
    "            \"action\": torch.tensor(action),\n",
    "            \"reward\": torch.tensor(reward),\n",
    "            \"next_state\": torch.tensor(np.array(next_state), dtype=torch.float32),\n",
    "            \"done\": torch.tensor(done)\n",
    "        }, batch_size=[])\n",
    "        self.replay_buffer.add(data)\n",
    "    \n",
    "    # copy weights of online network to target network if enough steps have passed\n",
    "    def sync_networks(self):\n",
    "        if self.learn_step_counter % self.sync_network_rate == 0 and self.learn_step_counter > 0:\n",
    "            self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "\n",
    "    # save current model (in case something goes wrong)\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.online_network.state_dict(), path)\n",
    "\n",
    "    # load model\n",
    "    def load_model(self, path):\n",
    "        self.online_network.load_state_dict(torch.load(path))\n",
    "        self.target_network.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        # if not enough experiences, return and keep going\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # copy weights to target network\n",
    "        self.sync_networks()\n",
    "        \n",
    "        # clear gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sample the replay buffer and store the results\n",
    "        samples = self.replay_buffer.sample(self.batch_size).to(self.online_network.device)\n",
    "        states = samples['state']\n",
    "        actions = samples['action']\n",
    "        rewards = samples['reward']\n",
    "        next_states = samples['next_state']\n",
    "        dones = samples['done']\n",
    "\n",
    "        # get the predicted values from our neural network with the appropriate batch size\n",
    "        predicted_q_values = self.online_network(states)\n",
    "        predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
    "\n",
    "        # Max returns two tensors, the first one is the maximum value, the second one is the index of the maximum value\n",
    "        target_q_values = self.target_network(next_states).max(dim=1)[0]\n",
    "        # The rewards of any future states don't matter if the current state is a terminal state\n",
    "        # If done is true, then 1 - done is 0, so the part after the plus sign (representing the future rewards) is 0\n",
    "        target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
    "\n",
    "        loss = self.loss(predicted_q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "        self.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058d5e2",
   "metadata": {},
   "source": [
    "This class performs an approximate mathematical implementation of the Bellman Equation for calculating Q-values for each state-action pair:\n",
    "\n",
    "$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$\n",
    "\n",
    "As seen here:\n",
    "\n",
    "```python\n",
    "target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
    "```\n",
    "\n",
    "PyTorch's built-in Mean Squared Error loss function is applied between predicted and target Q-values:\n",
    "```python\n",
    "self.losss = torch.nn.MSE()\n",
    "```\n",
    "```python\n",
    "loss = self.loss(predicted_q_values, target_q_values)\n",
    "```\n",
    "\n",
    "This is then minimized using backpropagation in\n",
    "```python\n",
    "loss.backward(); self.optimizer.step() in learn())`.\n",
    "```\n",
    "\n",
    "The agent selects actions using an epsilon-greedy policy  as seen in the `choose_action()` function, where the epsilon value represents the probability the model chooses a random action, selecting the action with the highest Q-value the rest of the time. This epsilon value decays overtime as seen here:\n",
    "\n",
    "```python\n",
    "self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min`\n",
    "```\n",
    "\n",
    "To stabilize learning, the agent periodically synchronizes the target network with the online network `self.sync_networks()`, copying the weights from the online network to the target network after a certain number of steps.\n",
    "\n",
    "We now have everything we need to create the environment and run our reinforcement learning architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7554543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:52: SyntaxWarning: invalid escape sequence '\\q'\n",
      "<>:52: SyntaxWarning: invalid escape sequence '\\q'\n",
      "C:\\Users\\sethe\\AppData\\Local\\Temp\\ipykernel_33160\\3476472889.py:52: SyntaxWarning: invalid escape sequence '\\q'\n",
      "  agent.save_model(os.path.join(\"trained_agents\\q_learning\",str(i + 1) + \"_q_agent.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 383.50 MB\n",
      "Currently processing episode 2/5 [################------------------------] 40.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sethe\\AppData\\Local\\Temp\\ipykernel_33160\\3476472889.py:52: SyntaxWarning: invalid escape sequence '\\q'\n",
      "  agent.save_model(os.path.join(\"trained_agents\\q_learning\",str(i + 1) + \"_q_agent.pt\"))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m new_state, reward, done, info  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[0;32m     46\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_in_memory(state, a, reward, new_state, done)\n\u001b[1;32m---> 47\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m SAVE_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[3], line 127\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m target_q_values \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m    126\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(predicted_q_values, target_q_values)\n\u001b[1;32m--> 127\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn_step_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
    "SAVE_INTERVAL = 1000\n",
    "DISPLAY = True\n",
    "NUM_OF_EPISODES = 100000\n",
    "\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right','A','B'],\n",
    "    ['right','B']\n",
    "]\n",
    "\n",
    "env = gym_super_mario_bros.make(ENV_NAME)\n",
    "env = JoypadSpace(env, CUSTOM_ACTIONS)\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "\n",
    "# create a progress bar when training\n",
    "def print_progress(cur,end, bar_length=40):\n",
    "    progress = cur / end\n",
    "    block = int(round(bar_length * progress))\n",
    "    text = f\"\\rCurrently processing episode {cur}/{end} [{'#' * block + '-' * (bar_length - block)}] {progress * 100:.2f}%\"\n",
    "    print(text, end='', flush=True)\n",
    "\n",
    "for i in range(NUM_OF_EPISODES):\n",
    "    print_progress(i+1,NUM_OF_EPISODES)\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        a = agent.choose_action(state)\n",
    "        new_state, reward, done, info  = env.step(a)\n",
    "        \n",
    "        agent.store_in_memory(state, a, reward, new_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        # save the model every save interval\n",
    "        if (i + 1) % SAVE_INTERVAL == 0:\n",
    "            agent.save_model(os.path.join(\"trained_agents\\q_learning\",str(i + 1) + \"_q_agent.pt\"))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3a1bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:568: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:619: UserWarning: \u001b[33mWARN: Env check failed with the following message: Calling the reset method with `return_info=True` did not return a 2-tuple\n",
      "You can set `disable_env_checker=True` to disable this check.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 612.57 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sethe\\AppData\\Local\\Temp\\ipykernel_20728\\3851061806.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.online_network.load_state_dict(torch.load(path))\n",
      "C:\\Users\\sethe\\AppData\\Local\\Temp\\ipykernel_20728\\3851061806.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.target_network.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 1433\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
    "SHOULD_TRAIN = True\n",
    "DISPLAY = True\n",
    "NUM_OF_EPISODES = 5\n",
    "\n",
    "CUSTOM_ACTIONS = [\n",
    "    ['right', 'A', 'B'],\n",
    "    ['right','B']\n",
    "]\n",
    "\n",
    "env = gym_super_mario_bros.make(ENV_NAME)\n",
    "env = JoypadSpace(env, CUSTOM_ACTIONS)\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "\n",
    "agent.load_model(os.path.join(\"trained_agents\",\"q_learning\",\"8000_q_agent.pt\"))\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "state = env.reset()\n",
    "for step in range(1500):\n",
    "    action = agent.choose_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['world'] == max_world and info['stage'] == max_stage and info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7fa66",
   "metadata": {},
   "source": [
    "![image](images/q_learning_4000.gif)\n",
    "\n",
    "WIth only 4,000 steps, the Q Learning algorithm is able to take advantage of the height reward and is clearing the tall pipes. Given enough iterations, I believe this algorithm would show similar, if not better results than our PPO algorithm. However, it took the same amount of time for me to run this algorithm for 4,000 iterations as it did for me to run the PPO algorithm for 600,000 steps. It is interesting to note the immediacy of the rewards when the PPO agent was still unable to clear any pipes at the 4,000th iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28095c2-22be-4762-812f-d5210cd89f2b",
   "metadata": {},
   "source": [
    "<a id='results'></a>\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28409ff",
   "metadata": {},
   "source": [
    "\n",
    "*show the result and interpretation of your experiment. Any iterative improvements summary.*\n",
    "\n",
    "5. Does it include the results summary, interpretation of experiments and visualization (e.g. performance comparison table, graphs etc)? (7)\n",
    "\n",
    "\n",
    "| Model     | Video of Model Predictions  | Notes    |Max World       | Max Level | Max X Position | Number of Training Steps  |\n",
    "|------------------|---- |---------------|----------------|-----------|-----------|-----------|\n",
    "| __Random Agent__     |  ![image](images/random_agent.gif)   |    Environment is working, but agent cannot progress far with random actions.        | 1            | 1         | 595         | N/A | \n",
    "| __Heuristic Agent__  | ![image](images/heuristic_agent.gif)   |  Agent wins using strategy and luck (not reinforcement learning)         | 1            | 2         | 199         | N/A |\n",
    "| __PPO Agent__        |  ![image](images/ppo_final.gif)   | Refining reward function keeps the agent from exploiting default rewards with unintended behavior        | 1            | 2         | 482         | 600,000 |\n",
    "| __Q Learning Agent__ | ![images](images/q_learning_4000.gif)   | Model is stable, but very slow         | 1            | 1         | 1423         | 5,000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455bdd-f926-4764-83ef-dbd6fc961590",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e061287",
   "metadata": {},
   "source": [
    "\n",
    "*Conclusion, discussion, reflection, or suggestions for future improvements or future ideas.*\n",
    "\n",
    "6. Does it include discussion (what went well or not and why), and suggestions for improvements or future work? (5)\n",
    "\n",
    "In the end the heuristic created a strong strategy for simplifying the model's action choices, and creating a set of custom actions to either run right, or run and jump right encouraged the model to control Mario similar to a real player, jumping over most obstacles and minimizing the chances for death by running into enemies on the ground.\n",
    "\n",
    "Utilizing the custom reward function to encourage higher jumping and creating the simplified action list allowed both the PPO and Q-Learning agents to progress much farther than initially, and ensured stable learning that balanced both exploration and exploitation. Prior to these revisions, the model continuously exploited the reward mechanics to a point that it was not progressing further into the game and would eventually converge on a policy that either stayed in one position (either stuck at a pipe or jumping over and over) or killing itself to the firts goomba repeatedly.\n",
    "\n",
    "The main difference in performance between the Q-Learning and PPO models was mainly down to processing complexity. The PPO model was simpler and was able to perform hundreds of thousands of training episodes in only a few hours, whereas the Q-Learning algorithm, while it was significantly more stable, took over 24 hours just to get to 50,000 training episodes. The benefits of the processing speed of Stable Baseline3's PPO library enabled me to see progress quickly and make adjustments as needed, whereas it was relatively difficult to figure out any problems with the Q-Learning algorithm until a considerable amount of time was wasted.\n",
    "\n",
    "Because of the processing drawbacks, I was forced to consider further optimizations to improve performance which I could use to speed up the SB3 PPO model even more.\n",
    "\n",
    "At the same time, I was able to implement the improvements made to the reward and action choices of the PPO model to further improve the Q-Learning model. Insights which would have taken days and weeks to gather if I simply used the Q-Learning agent alone.\n",
    "\n",
    "It would be interesting to see how much the Q-Learning agent would improve given the same number of iterations the PPO model was able to make and compare them side by side. Additionally, I have seen other projects using this model that created custom wrappers that are able to read the memory of the Super Mario Bros environment to be able to have even more information for the agent to utilize, effectively being able to determine locations of enemies, blocks, pits, etc.\n",
    "\n",
    "Given additional time, I like to see how well the agent would be able to improve with this information. This in combination with using random stage selection could also allow the model to be able to create a more generalized optimal policy that could assist in beating many levels of the game. As it is, my model seems to be making progress very slowly as it learns from trial and error as opposed to dynamically \"seeing\" information in the game state and choosing actions accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6817d-c989-41d6-9a06-2c128b50ae0a",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "\n",
    "## References\n",
    "\n",
    "*Reference: Please include all relevant links (git, video, etc)*\n",
    "\n",
    "7. Does it include all deliverables (3)\n",
    "\t- git with codes or notebooks\n",
    "\t- writeup (you can consider notebook as a writeup if the notebook contains all needed contents and explanation)\n",
    "\t- demo clips\n",
    "\t- proper quote or reference\n",
    "    \n",
    "Kauten, C. (2018). Super Mario Bros for OpenAI Gym. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "NathanGavenski. (2023). Comment on issue #128 in Kautenja/gym-super-mario-bros repository. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091\n",
    "\n",
    "Kundu, S. (2023, October 2). Train AI to beat Super Mario Bros! || Reinforcement learning completely from scratch [Video]. YouTube. https://www.youtube.com/watch?v=_gmQZToTMac\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

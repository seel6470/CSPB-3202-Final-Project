{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffc2037-5366-430d-bb52-2170e163d598",
   "metadata": {},
   "source": [
    "GitHub Repository for this project can be found at https://github.com/seel6470/CSPB-3202-Final-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8866730-fe6d-4e2f-ba9d-61204c3f1aa0",
   "metadata": {},
   "source": [
    "# Super Mario Bros Reflexive Agent\n",
    "<ol type=\"I\">\n",
    "<li><a href='#short-overview'>Short Overview</a></li>\n",
    "<li><a href='#approach'>Approach</a>\n",
    "  <ol type=\"A\">\n",
    "    <li><a href='#random-agent'>Random Agent</a></li>\n",
    "    <li><a href='#heuristic-agent'>Heuristic Agent</a></li>\n",
    "    <li><a href='#basic-ppo-agent'>Basic PPO Agent</a></li>\n",
    "    <li><a href='#q-learning-agent'>Q Learning Agent</a></li>\n",
    "  </ol>\n",
    "</li>\n",
    "<li><a href='#results'>Results</a></li>\n",
    "<li><a href='#conclusion'>Conclusion</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99cf0f-1b67-44ab-a413-64bd96974842",
   "metadata": {},
   "source": [
    "<a id='overview'></a>\n",
    "### Short Overview\n",
    "\n",
    "*short overview of what your project is about (e.g. you're building /testing certain RL models in certain environments; yes you can test your algorithm in more than 1 environment if your goal is to test an algorithm(s) performances in different settings)*\n",
    "\n",
    "1. Does it include the clear overview on what the project is about? (4)\n",
    "\n",
    "2. Does it explain how the environment works and what the game rules are? (4)\n",
    "\n",
    "For my project, I chose to teach a learning model to play the original Super Mario Bros. game for the NES. I utilized a library created by Christian Kauten called gym-super-mario-bros, which provides an OpenAI Gym environment using the nes-py emulator (Kauten, 2018). The challenge is to beat as many levels as possible in the original Mario game for NES with the following rules of the game.\n",
    "\n",
    "The goal of the game is to avoid enemies and pits to reach the end of each level. One hit and Mario loses a life, starting over from the nearest checkpoint. Power-ups provide Mario an additional hit. The following page from the original game manual outlines the inputs Mario receives for the game:\n",
    "\n",
    "![image](images/controls.jpg)\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "The game environment takes these controls and creates the following action lists that can be used within the environment wrapper:\n",
    "\n",
    "```python\n",
    "# actions for the simple run right environment\n",
    "RIGHT_ONLY = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for very simple movement\n",
    "SIMPLE_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "]\n",
    "\n",
    "\n",
    "# actions for more complex movement\n",
    "COMPLEX_MOVEMENT = [\n",
    "    ['NOOP'],\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['A'],\n",
    "    ['left'],\n",
    "    ['left', 'A'],\n",
    "    ['left', 'B'],\n",
    "    ['left', 'A', 'B'],\n",
    "    ['down'],\n",
    "    ['up'],\n",
    "]\n",
    "```\n",
    "\n",
    "The environment can also determine the following keys for the gamestate:\n",
    "\n",
    "| Key       | Type | Description                                |\n",
    "|-----------|------|--------------------------------------------|\n",
    "| coins     | int  | The number of collected coins              |\n",
    "| flag_get  | bool | True if Mario reached a flag or ax         |\n",
    "| life      | int  | The number of lives left, i.e., {3, 2, 1}  |\n",
    "| score     | int  | The cumulative in-game score               |\n",
    "| stage     | int  | The current stage, i.e., {1, ..., 4}       |\n",
    "| status    | str  | Mario's status, i.e., {'small', 'tall', 'fireball'} |\n",
    "| time      | int  | The time left on the clock                 |\n",
    "| world     | int  | The current world, i.e., {1, ..., 8}       |\n",
    "| x_pos     | int  | Mario's x position in the stage (from the left) |\n",
    "| y_pos     | int  | Mario's y position in the stage (from the bottom) |\n",
    "\n",
    "Additionally, the environment utilizes the following parameters for the reward function:\n",
    "\n",
    "v: the difference in agent x values between states\n",
    "\n",
    "c: the difference in the game clock between frames\n",
    "\n",
    "d: a death penalty that penalizes the agent for dying in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aeec23-5c3b-4af6-a8c8-ee157900fc45",
   "metadata": {},
   "source": [
    "<a id='approach'></a>\n",
    "\n",
    "# Approach\n",
    "\n",
    "*explain your environment, your choice of model(s), the methods and purpose of testing and experiments, explain any trouble shooting required.*\n",
    "\n",
    "3. Does it explain clearly the model(s) of choices, the methods and purpose of tests and experiments? (7)\n",
    "\n",
    "4. Does it show problem solving procedure- e.g. how the author solved and improved when an algorithm doesn't work well. Note that it's not about debugging or programming/implementation, but about when a correctly implemented algorithm wasn't enough for the problem and the author had to modify/add some features or techniques, or compare with another model, etc. (7)\n",
    "\n",
    "The initial setup for the environment was a bit tricky due to some incompatibilities between the chosen gym library gym-super-mario-bros JoypadSpace wrapper and the current version of OpenAi's gym framework, specifically with the `reset` method. Huge thanks to NathanGavinski who supplied [a workaround](https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091) in the issues forum for gym-super-mario-bros Git. (NathanGavenski, 2023).\n",
    "\n",
    "The following code utilizes this fix along with the suggested boilerplate setup from the gym-super-mario-bros documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05dc07bf-8674-4287-878a-569fd9ab9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gymnasium.wrappers import StepAPICompatibility, TimeLimit\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2b346",
   "metadata": {},
   "source": [
    "### Random Agent\n",
    "\n",
    "Let's create an agent that makes random movements just to make sure our environment is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de8bc496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 595\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "done = True\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "# Run the environment for 5000 steps\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    done = done or truncated\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.05 seconds between frames\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27aa4a",
   "metadata": {},
   "source": [
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"./images/random_agent.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "You can see that the random agent never gets past the second pipe. This is because it is not probabilistically reasonable for random inputs to know to sustain a jump by pressing A to get high enough to clear the pipe and keep going. This pipe exists at an X value of 595. Let's see if there are any other agents that can get farther."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a4f41",
   "metadata": {},
   "source": [
    "## Heuristic Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489cad9-4b55-4b39-bf99-e12bab7547da",
   "metadata": {},
   "source": [
    "To get a baseling, I decided to implement a basic heuristic model that uses a simple algorithm to try to beat a level of Super Mario Bros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7347e250-b94d-4483-8810-fdcfe114b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 320\n",
      "Max World: 1\n",
      "Max Stage: 2\n"
     ]
    }
   ],
   "source": [
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Define a new reset function to accept `seeds` and `options` args for compatibility\n",
    "def gymnasium_reset(self, **kwargs):\n",
    "    return self.env.reset(), {}\n",
    "\n",
    "# Overwrite the old reset to accept `seeds` and `options` args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)\n",
    "\n",
    "# create global variables for inputs\n",
    "done = True\n",
    "going_up = False\n",
    "prev_y = None\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "for step in range(1700):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        prev_y = None\n",
    "        hold_jump = False\n",
    "    \n",
    "    # if Mario is on flat groun\n",
    "    # or in the process of rising from previous jump\n",
    "    # will continue to hold A to perform the maximum jump\n",
    "    action = SIMPLE_MOVEMENT.index(['right', 'A', 'B']) if going_up else SIMPLE_MOVEMENT.index(['right', 'B'])\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # set going_up to true if Mario is not descending\n",
    "    if prev_y is not None:\n",
    "        if info['y_pos'] >= prev_y:\n",
    "            going_up = True\n",
    "        else:\n",
    "            going_up = False\n",
    "\n",
    "    # capture current y position to compare for next state\n",
    "    prev_y = info['y_pos']\n",
    "        \n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "\n",
    "    if done or truncated:\n",
    "        done = True\n",
    "    env.render()\n",
    "    time.sleep(0.01)  # Add a delay of 0.01 seconds between frames\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4640cca",
   "metadata": {},
   "source": [
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"./images/heuristic_agent.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "This heuristic, while not an actual learning model, is effective by sheer luck. This strategy is similar to what an actual player might try when attempting to beat the first level. It is intuitive to try and jump as high and as often as possible to clear most obstacles and enemies. It is by sheer luck (and some deaths) that the heuristic is able to avoid enemies and pits. Despite this, however, this agent is able to clear the first level, although it does not get very far in the second level.\n",
    "\n",
    "Let's try to create a learning model and see if the agent can beat the level without relying on luck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4371",
   "metadata": {},
   "source": [
    "## PPO Agent\n",
    "\n",
    "Let's actually have an agent that learns, rather than blindly jumping. We'll start by creating a basic Proximal Policy Optimization (PPO) that makes stable updates to policies by limiting drastic changes, ensuring efficient learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9957fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  raise ModuleNotFoundError(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\sethe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 103 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 1   |\n",
      "|    total_timesteps | 128 |\n",
      "----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 256         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033081587 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | -0.00124    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 192         |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 425         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 20        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 18        |\n",
      "|    total_timesteps      | 384       |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0200424 |\n",
      "|    clip_fraction        | 0.0996    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.9      |\n",
      "|    explained_variance   | 0.0105    |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 20.4      |\n",
      "|    n_updates            | 8         |\n",
      "|    policy_gradient_loss | -0.00349  |\n",
      "|    value_loss           | 168       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 512        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06563064 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.85      |\n",
      "|    explained_variance   | 0.069      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 45.8       |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | 0.0115     |\n",
      "|    value_loss           | 98.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 640         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019646216 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 34.6        |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 0.000365    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 47         |\n",
      "|    total_timesteps      | 768        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16857654 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.53      |\n",
      "|    explained_variance   | -0.0736    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 172        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.0216     |\n",
      "|    value_loss           | 247        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 896          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056644334 |\n",
      "|    clip_fraction        | 0.0801       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.00282     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 36.8         |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    value_loss           | 145          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015888235 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.0211     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 58.4        |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.000523   |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 1152         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054389173 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -0.00162     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 5.19         |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    value_loss           | 44.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 1280         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021641203 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.0177      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | 0.00707      |\n",
      "|    value_loss           | 70.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 1408        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015707789 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 0.0157      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.93        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 23          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 103        |\n",
      "|    total_timesteps      | 1536       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00930596 |\n",
      "|    clip_fraction        | 0.0215     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | -0.0179    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 7.64       |\n",
      "|    n_updates            | 44         |\n",
      "|    policy_gradient_loss | -0.00553   |\n",
      "|    value_loss           | 37.8       |\n",
      "----------------------------------------\n",
      "Mean reward: 680.0 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "env = StepAPICompatibility(env, output_truncation_bool=True)\n",
    "\n",
    "model = PPO(\n",
    "    'CnnPolicy',      # Use a convolutional neural network\n",
    "    env,              # environment\n",
    "    verbose=1,        # print diagnostics\n",
    "    learning_rate=1e-4,  # controls how much to adjust the model with each step\n",
    "    n_steps=128,      # affects the frequency of updates\n",
    "    batch_size=64,    # number of samples per gradient update\n",
    "    n_epochs=4,       # Number of epochs\n",
    "    clip_range=0.2,   # helps in limiting updates for stable training\n",
    "    ent_coef=0.03     # use entropy to encourage exploration\n",
    ")\n",
    "\n",
    "# Define evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=500, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./models/', name_prefix='ppo_mario')\n",
    "\n",
    "model.learn(total_timesteps=1500)\n",
    "\n",
    "# model = PPO.load(\"ppo_mario.zip\")\n",
    "\n",
    "model.save(\"./trained_agents/ppo_mario\")\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6daca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max X: 595\n",
      "Max World: 1\n",
      "Max Stage: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import time\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import StepAPICompatibility, TimeLimit\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "# Create the Super Mario Bros. environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "steps = env._max_episode_steps  # get the original max_episode_steps count\n",
    "\n",
    "# Set the Joypad wrapper\n",
    "env = JoypadSpace(env.env, SIMPLE_MOVEMENT)\n",
    "# Overwrite the old reset to accept seeds and options args\n",
    "env.reset = gymnasium_reset.__get__(env, JoypadSpace)\n",
    "\n",
    "# Set TimeLimit back\n",
    "env = TimeLimit(StepAPICompatibility(env, output_truncation_bool=True), max_episode_steps=steps)\n",
    "\n",
    "# model = PPO.load(\"ppo_mario.zip\")\n",
    "\n",
    "max_x = 0\n",
    "max_world = 0\n",
    "max_stage = 0\n",
    "\n",
    "\n",
    "obs, info = env.reset()\n",
    "for step in range(1500):\n",
    "    action, _states = model.predict(obs.copy())\n",
    "    action = action.item()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if info['world'] > max_world:\n",
    "        max_world = info['world']\n",
    "        max_x = 0\n",
    "        max_stage = 1\n",
    "    elif info['stage'] > max_stage:\n",
    "        max_stage = info['stage']\n",
    "        max_x = 0\n",
    "    elif info['x_pos'] > max_x:\n",
    "        max_x = info['x_pos']\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Max X: {max_x}\")\n",
    "print(f\"Max World: {max_world}\")\n",
    "print(f\"Max Stage: {max_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4def3",
   "metadata": {},
   "source": [
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"./images/PPO_agent.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "This PPO agent is, unfortunately, unable to clear the pipe at the 595 X value. This fairs about as well as the random agent, and not as good as the heuristic agent. Given more timesteps, the agent could learn how to get farther, however let us try to create better reflexive learning agent using the Q learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3c39e",
   "metadata": {},
   "source": [
    "### Q-Learning Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c92186",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c28095c2-22be-4762-812f-d5210cd89f2b",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "*show the result and interpretation of your experiment. Any iterative improvements summary.*\n",
    "\n",
    "5. Does it include the results summary, interpretation of experiments and visualization (e.g. performance comparison table, graphs etc)? (7)\n",
    "\n",
    "# Agent Performance Table\n",
    "\n",
    "| Name of Agent    | Max Level | Max World | Max X Position |\n",
    "|------------------|----------------|-----------|-----------|\n",
    "| Random Agent     | 1            | 1         | 595         |\n",
    "| Heuristic Agent  | 1            | 2         | 320         |\n",
    "| Basic PPO Agent  | 1            | 1         | 595         |\n",
    "| Q Learning Agent | 200            | 1         | 4         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455bdd-f926-4764-83ef-dbd6fc961590",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "*Conclusion, discussion, reflection, or suggestions for future improvements or future ideas.*\n",
    "\n",
    "6. Does it include discussion (what went well or not and why), and suggestions for improvements or future work? (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6817d-c989-41d6-9a06-2c128b50ae0a",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "*Reference: Please include all relevant links (git, video, etc)*\n",
    "\n",
    "7. Does it include all deliverables (3)\n",
    "\t- git with codes or notebooks\n",
    "\t- writeup (you can consider notebook as a writeup if the notebook contains all needed contents and explanation)\n",
    "\t- demo clips\n",
    "\t- proper quote or reference\n",
    "    \n",
    "Kauten, C. (2018). Super Mario Bros for OpenAI Gym. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros\n",
    "\n",
    "Nintendo. (1985). Super Mario Bros. Instruction Manual. Nintendo of America Inc. Retrieved from [https://www.nintendo.co.jp/clv/manuals/en/pdf/CLV-P-NAAAE.pdf]\n",
    "\n",
    "NathanGavenski. (2023). Comment on issue #128 in Kautenja/gym-super-mario-bros repository. GitHub. Retrieved from https://github.com/Kautenja/gym-super-mario-bros/issues/128#issuecomment-1954019091\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b94f9b-15f1-48eb-8b0d-8584b7a585a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
